"""
ë¡œì»¬ LLM í•¸ë“¤ëŸ¬ (Ollama/Qwen 2.5)
ì˜¨ë””ë°”ì´ìŠ¤ ì„ë² ë”© ëª¨ë¸ í¬í•¨
"""

import asyncio
from typing import Optional
import httpx
import numpy as np

from app.config import settings
from app.logger import get_logger
from .model_factory import BaseLLM

logger = get_logger(__name__)

# Cloud Run CPU í™˜ê²½ì—ì„œ LLM ì‘ë‹µ ì‹œê°„ ê³ ë ¤ (ìµœëŒ€ 5ë¶„)
OLLAMA_TIMEOUT = 300.0


class OllamaClient:
    """Ollama API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, base_url: Optional[str] = None, model: Optional[str] = None):
        self.base_url = base_url or settings.OLLAMA_BASE_URL
        self.model = model or settings.OLLAMA_MODEL
        self._client: Optional[httpx.AsyncClient] = None
    
    async def _get_client(self) -> httpx.AsyncClient:
        """HTTP í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        if self._client is None:
            self._client = httpx.AsyncClient(timeout=OLLAMA_TIMEOUT)
        return self._client
    
    async def generate(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            temperature: ìƒì„± ì˜¨ë„
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
        
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        client = await self._get_client()
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature or settings.LLM_TEMPERATURE,
                "num_predict": max_tokens or settings.LLM_MAX_TOKENS,
            }
        }
        
        response = await client.post(
            f"{self.base_url}/api/generate",
            json=payload
        )
        response.raise_for_status()
        
        return response.json()["response"]
    
    async def chat(
        self,
        messages: list[dict],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        """
        ì±„íŒ… í˜•ì‹ ìƒì„±
        
        Args:
            messages: ì±„íŒ… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            temperature: ìƒì„± ì˜¨ë„
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
        
        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        client = await self._get_client()
        
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": temperature or settings.LLM_TEMPERATURE,
                "num_predict": max_tokens or settings.LLM_MAX_TOKENS,
                "repeat_penalty": 1.3,  # ë°˜ë³µ ë°©ì§€ (íŒŒì¸íŠœë‹ ëª¨ë¸ ê³¼ì í•© ë³´ì™„)
                "top_p": 0.9,
                "top_k": 40,
            }
        }
        
        try:
            response = await client.post(
                f"{self.base_url}/api/chat",
                json=payload
            )
            response.raise_for_status()
            content = response.json()["message"]["content"]
            
            # EXAONE 4.0 í›„ì²˜ë¦¬
            content = self._postprocess_exaone(content)
            
            return content
        
        except httpx.TimeoutException:
            logger.error(f"Ollama íƒ€ì„ì•„ì›ƒ ({OLLAMA_TIMEOUT}ì´ˆ ì´ˆê³¼)")
            return "ì£„ì†¡í•©ë‹ˆë‹¤, ì‘ë‹µ ìƒì„±ì— ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê³  ìˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”. ğŸ™"
        
        except httpx.HTTPError as e:
            logger.error(f"Ollama HTTP ì—ëŸ¬: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤, ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”. ğŸ™"
    
    @staticmethod
    def _postprocess_exaone(content: str) -> str:
        """EXAONE 4.0 ì‘ë‹µ í›„ì²˜ë¦¬: think ë¸”ë¡ ì œê±°, ì•„í‹°íŒ©íŠ¸ ì •ë¦¬"""
        import re
        
        # 1) <think>...</think> ë¸”ë¡ ì œê±° (ë¹„ì¶”ë¡  ëª¨ë“œ ë¹ˆ ë¸”ë¡ í¬í•¨)
        if "</think>" in content:
            content = content.split("</think>", 1)[1]
        content = re.sub(r"<think>.*?</think>", "", content, flags=re.DOTALL)
        content = re.sub(r"</?think>", "", content)
        
        # 2) <tool_call> ë“± hallucinated íƒœê·¸ ì œê±°
        content = re.sub(r"</?tool_call[^>]*>", "", content)
        
        # 3) ì•ë’¤ ë¶ˆí•„ìš”í•œ ë¬¸ì ì •ë¦¬ (`:`, `[-1]` ë“±)
        content = content.strip()
        content = re.sub(r"^\[[-\d]*\]\s*", "", content)  # [-1] ë“±
        content = re.sub(r"^:\s*", "", content)  # ì•ì˜ : ì œê±°
        
        return content.strip()
    
    async def is_available(self) -> bool:
        """Ollama ì„œë²„ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            client = await self._get_client()
            response = await client.get(f"{self.base_url}/api/tags")
            return response.status_code == 200
        except Exception:
            return False
    
    async def close(self):
        """í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ"""
        if self._client:
            await self._client.aclose()
            self._client = None


class LocalEmbedding:
    """ì˜¨ë””ë°”ì´ìŠ¤ ì„ë² ë”© ëª¨ë¸ (sentence-transformers)"""
    
    _instance: Optional["LocalEmbedding"] = None
    _model = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if self._model is None:
            # lazy import + safetensors LOAD REPORT ì–µì œ (fd ë¦¬ë‹¤ì´ë ‰íŠ¸)
            import os as _os
            _devnull = _os.open(_os.devnull, _os.O_WRONLY)
            _old_stdout = _os.dup(1)
            _old_stderr = _os.dup(2)
            _os.dup2(_devnull, 1)
            _os.dup2(_devnull, 2)
            try:
                from sentence_transformers import SentenceTransformer
                self.__class__._model = SentenceTransformer(
                    settings.EMBEDDING_MODEL,
                    device=settings.EMBEDDING_DEVICE
                )
            finally:
                _os.dup2(_old_stdout, 1)
                _os.dup2(_old_stderr, 2)
                _os.close(_old_stdout)
                _os.close(_old_stderr)
                _os.close(_devnull)
    
    def embed(self, texts: list[str]) -> np.ndarray:
        """
        í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì„ë² ë”© ë²¡í„° ë°°ì—´ (shape: [n, 384])
        """
        embeddings = self._model.encode(
            texts,
            convert_to_numpy=True,
            show_progress_bar=False
        )
        return embeddings
    
    def embed_query(self, query: str) -> list[float]:
        """
        ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        
        Args:
            query: ì¿¼ë¦¬ í…ìŠ¤íŠ¸
        
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        embedding = self._model.encode(query, convert_to_numpy=True)
        return embedding.tolist()
    
    def embed_documents(self, documents: list[str]) -> list[list[float]]:
        """
        ë¬¸ì„œ ì„ë² ë”© ìƒì„±
        
        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        embeddings = self._model.encode(documents, convert_to_numpy=True)
        return embeddings.tolist()
    
    @property
    def dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì›"""
        return settings.EMBEDDING_DIMENSION


class LocalLLM(BaseLLM):
    """ë¡œì»¬ LLM (Ollama ê¸°ë°˜)"""
    
    def __init__(self):
        self.client = OllamaClient()
        self._embedding = LocalEmbedding()
    
    async def generate(self, prompt: str, **kwargs) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        return await self.client.generate(prompt, **kwargs)
    
    async def chat(self, messages: list[dict], **kwargs) -> str:
        """ì±„íŒ… í˜•ì‹ ìƒì„±"""
        return await self.client.chat(messages, **kwargs)
    
    async def is_available(self) -> bool:
        """ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€"""
        return await self.client.is_available()
    
    @property
    def embedding(self) -> LocalEmbedding:
        """ì„ë² ë”© ëª¨ë¸"""
        return self._embedding


# ì‹±ê¸€í†¤ ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤
def get_embedding_model() -> LocalEmbedding:
    """ì„ë² ë”© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    return LocalEmbedding()

"""
GraphRAG ê²€ìƒ‰ê¸° (Hybrid Keyword Extraction)
ì§€ì‹ê·¸ë˜í”„ + ë²¡í„° ê²€ìƒ‰ì„ ê²°í•©í•˜ì—¬ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œë‹¤.

í‚¤ì›Œë“œ ì¶”ì¶œ 3ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ:
  1ë‹¨ê³„: ê·œì¹™ ê¸°ë°˜ â€” ê·¸ë˜í”„ ë…¸ë“œëª… ì§ì ‘ë§¤ì¹­ + êµ¬ì–´â†’ì˜í•™ìš©ì–´ regex (ì¦‰ì‹œ, 0ms)
  2ë‹¨ê³„: ì„ë² ë”© ìœ ì‚¬ë„ â€” sentence-transformerë¡œ ë…¸ë“œ ì˜ë¯¸ ë§¤ì¹­ (+30ms)
  3ë‹¨ê³„: LLM Fallback â€” 1~2ë‹¨ê³„ ê²°ê³¼ ë¶€ì¡± ì‹œ Ollamaì— ì˜í•™ í‚¤ì›Œë“œ ì¶”ì¶œ ìš”ì²­ (+500ms~2s)
"""

import asyncio
import re
from typing import Optional

import numpy as np

from app.knowledge_graph.health_kg import (
    HealthKnowledgeGraph,
    get_health_kg,
    NodeType,
)
from app.config import settings
from app.logger import get_logger
from app.model.local_model import get_embedding_model

logger = get_logger(__name__)

# â”€â”€ ì„ë² ë”© ìœ ì‚¬ë„ ë§¤ì¹­ ì„¤ì • â”€â”€
# ë‹¤êµ­ì–´ ëª¨ë¸ìš© ì„ê³„ê°’
EMBEDDING_SIMILARITY_THRESHOLD = 0.55   # ì ˆëŒ€ ì„ê³„ê°’
EMBEDDING_TOP_K = 2                     # ìƒìœ„ Kê°œê¹Œì§€ í›„ë³´ ì„ ì •
EMBEDDING_GAP_THRESHOLD = 0.04          # 1ìœ„ì™€ 4ìœ„ ê°„ ìµœì†Œ ì ìˆ˜ ê°­ (ì‘ìœ¼ë©´ ë…¸ì´ì¦ˆ)


class GraphRAGRetriever:
    """ì§€ì‹ê·¸ë˜í”„ ê¸°ë°˜ RAG ê²€ìƒ‰ê¸° (í•˜ì´ë¸Œë¦¬ë“œ í‚¤ì›Œë“œ ì¶”ì¶œ)"""

    def __init__(self, kg: Optional[HealthKnowledgeGraph] = None):
        self.kg = kg or get_health_kg()
        # â”€â”€ ì„ë² ë”© ì¸ë±ìŠ¤ (ì•± ì‹œì‘ ì‹œ 1íšŒ êµ¬ì¶•) â”€â”€
        self._node_names: list[str] = []
        self._node_embeddings: Optional[np.ndarray] = None
        self._embedding = get_embedding_model()  # ê¸°ì¡´ ì‹±ê¸€í†¤ ì¬ì‚¬ìš©
        self._build_node_index()

    def _build_node_index(self):
        """ì§€ì‹ê·¸ë˜í”„ì˜ ëª¨ë“  ë…¸ë“œë¥¼ ì„ë² ë”©í•˜ì—¬ ìœ ì‚¬ë„ ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•œë‹¤."""
        try:
            # ë…¸ë“œëª…ë§Œ ì„ë² ë”© (ì„¤ëª… í¬í•¨ ì‹œ ì¼ë°˜ ë‹¨ì–´ê°€ ë…¸ì´ì¦ˆ ìœ ë°œ)
            node_texts = []
            for node, data in self.kg.graph.nodes(data=True):
                node_texts.append(node)
                self._node_names.append(node)

            self._node_embeddings = self._embedding.embed(node_texts)
            logger.info(
                f"ğŸ”— GraphRAG ë…¸ë“œ ì„ë² ë”© ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ: "
                f"{len(self._node_names)}ê°œ ë…¸ë“œ, "
                f"model={settings.EMBEDDING_MODEL}, "
                f"dim={self._node_embeddings.shape[1]}"
            )
        except Exception as e:
            logger.warning(f"ë…¸ë“œ ì„ë² ë”© ì¸ë±ìŠ¤ êµ¬ì¶• ì‹¤íŒ¨ (ê·œì¹™ ê¸°ë°˜+LLMë§Œ ì‚¬ìš©): {e}")
            self._node_embeddings = None

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ê²€ìƒ‰ ë©”ì¸ ì—”íŠ¸ë¦¬
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def search(self, query: str) -> str:
        """
        ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ì§€ì‹ê·¸ë˜í”„ë¥¼ íƒìƒ‰í•˜ì—¬
        êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ì„ ë°˜í™˜í•œë‹¤.
        """
        keywords = self._extract_keywords_hybrid(query)

        if not keywords:
            return ""

        context_parts = []

        for keyword in keywords:
            # 1. ì¦ìƒâ†’ì§ˆí™˜ ê²½ë¡œ íƒìƒ‰
            conditions = self.kg.get_symptom_conditions(keyword)
            for cond in conditions:
                info = self.kg.get_condition_info(cond)
                if info:
                    context_parts.append(self._format_condition_info(info))

            # 2. ì§ˆí™˜ ì§ì ‘ ë§¤ì¹­
            info = self.kg.get_condition_info(keyword)
            if info and info.get("symptoms"):
                context_parts.append(self._format_condition_info(info))

            # 3. ê´€ë ¨ ë…¸ë“œ íƒìƒ‰ (ê¹Šì´ 2)
            if not conditions and not info.get("symptoms", []):
                related = self.kg.find_related_nodes(keyword, depth=2)
                if related:
                    summary = self._format_related_nodes(keyword, related)
                    if summary:
                        context_parts.append(summary)

        # ì¤‘ë³µ ì œê±°
        unique_parts = list(dict.fromkeys(context_parts))
        result = "\n".join(unique_parts[:3])  # ìµœëŒ€ 3ê°œ ì§ˆí™˜ ì •ë³´

        if result:
            logger.info(f"ğŸ§  GraphRAG | keywords={keywords} | context_len={len(result)}")

        return result

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # í•˜ì´ë¸Œë¦¬ë“œ í‚¤ì›Œë“œ ì¶”ì¶œ (3ë‹¨ê³„)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _extract_keywords_hybrid(self, query: str) -> list[str]:
        """
        3ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ í‚¤ì›Œë“œ ì¶”ì¶œ:
        1) ê·œì¹™ ê¸°ë°˜ (ì§ì ‘ë§¤ì¹­ + regex)  â€” ~0ms
        2) ì„ë² ë”© ìœ ì‚¬ë„ ë§¤ì¹­            â€” ~30ms
        3) LLM Fallback                 â€” ~500ms~2s (í•„ìš”ì‹œë§Œ)
        """
        keywords = []

        # â”€â”€ Stage 1: ê·œì¹™ ê¸°ë°˜ (ê¸°ì¡´ ë°©ì‹, ë¹ ë¦„) â”€â”€
        rule_keywords = self._extract_by_rules(query)
        keywords.extend(rule_keywords)

        # â”€â”€ Stage 2: ì„ë² ë”© ìœ ì‚¬ë„ ë§¤ì¹­ â”€â”€
        embed_keywords = self._extract_by_embedding(query)
        for kw in embed_keywords:
            if kw not in keywords:
                keywords.append(kw)

        # â”€â”€ Stage 3: LLM Fallback (1~2ë‹¨ê³„ ê²°ê³¼ê°€ ë¶€ì¡±í•  ë•Œë§Œ) â”€â”€
        if len(keywords) == 0:
            llm_keywords = self._extract_by_llm(query)
            for kw in llm_keywords:
                if kw not in keywords:
                    keywords.append(kw)

        if keywords:
            logger.info(
                f"ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ | "
                f"rule={rule_keywords} | "
                f"embed={embed_keywords} | "
                f"final={keywords[:5]}"
            )

        return keywords[:5]  # ìµœëŒ€ 5ê°œ

    # â”€â”€ Stage 1: ê·œì¹™ ê¸°ë°˜ â”€â”€

    def _extract_by_rules(self, query: str) -> list[str]:
        """ê·¸ë˜í”„ ë…¸ë“œëª… ì§ì ‘ë§¤ì¹­ + êµ¬ì–´â†’ì˜í•™ìš©ì–´ regex íŒ¨í„´."""
        keywords = []

        # ë…¸ë“œëª… ì§ì ‘ ë§¤ì¹­
        for node in self.kg.graph.nodes:
            if node in query:
                keywords.append(node)

        # êµ¬ì–´ â†’ ì˜í•™ìš©ì–´ ë§¤í•‘
        colloquial_map = {
            r"ì ì„?\s*ëª»": "ë¶ˆë©´ì¦",
            r"ì ì´?\s*ì•ˆ": "ë¶ˆë©´ì¦",
            r"ë°¤ì—?\s*ê¹¨": "ìˆ˜ë©´ íŒ¨í„´ ë³€í™”",
            r"ë°œí†±.*(ì•ˆ|ì•ˆìª½|íŒŒê³ |ë“¤ì–´)": "ë‚´í–¥ì„± ë°œí†±",
            r"ë°œí†±.*(ë‘êº¼|ë³€í˜•|íœ˜)": "ë°œí†± ë³€í˜•",
            r"ì†Œë³€.*(ëª» ì°¸|ì‹¤ìˆ˜|ìì£¼)": "ìš”ì‹¤ê¸ˆ",
            r"ë¨¸ë¦¬.*(ë¹ ì§€|ë¹ ì ¸|íƒˆëª¨)": "ë¨¸ë¦¬ì¹´ë½ ë¹ ì§",
            r"í”¼ë¶€.*(ê°€ë ¤|ê±´ì¡°|íŠ¸ëŸ¬ë¸”)": "í”¼ë¶€ ê°€ë ¤ì›€",
            r"ìˆ¨.*(ì°¨|ì•ˆ ì‰¬|ëª» ì‰¬)": "í˜¸í¡ê³¤ë€",
            r"ê¸°ì¹¨.*(ì˜¤ë˜|ì•ˆ ë©ˆ|ê³„ì†)": "ë§Œì„±ê¸°ì¹¨",
            r"ì†.*(ì €ë¦¬|ê°ê°)": "ì†ë°œ ì €ë¦¼",
            r"ë°œ.*(ì €ë¦¬|ê°ê°)": "ì†ë°œ ì €ë¦¼",
            r"ê·€.*(ì•ˆ ë“¤|ì˜ ì•ˆ)": "ì†Œë¦¬ ì•ˆ ë“¤ë¦¼",
            r"ëˆˆ.*(ì¹¨ì¹¨|íë¦¿|ì•ˆ ë³´)": "ì‹œë ¥ ì €í•˜",
            r"ì‡ëª¸.*(í”¼|ì¶œí˜ˆ|ë¶“)": "ì‡ëª¸ ì¶œí˜ˆ",
            r"ë³€.*(ì•ˆ ë‚˜|ëª» ë³´|í˜ë“¤)": "ë°°ë³€ ê³¤ë€",
            r"ë°°.*(ë”ë¶€ë£©|íŒ½ë§Œ|ë¶€ë¥¸)": "ë³µë¶€ íŒ½ë§Œ",
            r"ê¸°ìš´.*(ì—†|ë¹ ì§€|ì €í•˜)": "ê¸°ë ¥ ì €í•˜",
            r"ë¼ˆ.*(ì•„í”„|í†µì¦|ì‘¤ì‹œ)": "ë¼ˆ í†µì¦",
            r"í˜ˆë‹¹.*(ë†’|ì˜¬ë¼)": "ê³ í˜ˆë‹¹",
            r"í˜ˆë‹¹.*(ë‚®|ë–¨ì–´)": "ì €í˜ˆë‹¹",
        }

        for pattern, medical_term in colloquial_map.items():
            if re.search(pattern, query):
                if medical_term not in keywords:
                    keywords.append(medical_term)

        return keywords

    # â”€â”€ Stage 2: ì„ë² ë”© ìœ ì‚¬ë„ ë§¤ì¹­ â”€â”€

    def _extract_by_embedding(self, query: str) -> list[str]:
        """
        ì¿¼ë¦¬ ì„ë² ë”©ê³¼ ê·¸ë˜í”„ ë…¸ë“œ ì„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ê´€ë ¨ ë…¸ë“œë¥¼ ì°¾ëŠ”ë‹¤.
        regexë¡œ ëª» ì¡ëŠ” í‘œí˜„ë„ ì˜ë¯¸ì ìœ¼ë¡œ ë§¤ì¹­ ê°€ëŠ¥.

        ì˜ˆ: "ë°¥ë§›ì´ ì—†ì–´" â†’ 'ì‹ìš•ë¶€ì§„' (cosine=0.52)
            "ì˜¨ëª¸ì´ ë»£ë»£í•´" â†’ 'ë¼ˆ í†µì¦' (cosine=0.48)
        """
        if self._node_embeddings is None:
            return []

        try:
            query_emb = self._embedding.embed([query])  # shape: (1, 384)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            # norm(query) * norm(nodes) â†’ dot product / norms
            query_norm = query_emb / (np.linalg.norm(query_emb, axis=1, keepdims=True) + 1e-10)
            node_norms = self._node_embeddings / (
                np.linalg.norm(self._node_embeddings, axis=1, keepdims=True) + 1e-10
            )
            similarities = (query_norm @ node_norms.T).flatten()  # (num_nodes,)

            # ìƒìœ„ Kê°œ ì¶”ì¶œ + ì ˆëŒ€ ì„ê³„ê°’ + ìƒëŒ€ ê°­ í•„í„°
            sorted_indices = np.argsort(similarities)[::-1]
            top_indices = sorted_indices[:EMBEDDING_TOP_K]

            # ìƒëŒ€ ê°­ ì²´í¬: 1ìœ„ ì ìˆ˜ê°€ 4ìœ„ ì ìˆ˜ë³´ë‹¤ ì¶©ë¶„íˆ ë†’ì€ì§€
            # (ê°­ì´ ì‘ìœ¼ë©´ ëª¨ë“  ë…¸ë“œê°€ ë¹„ìŠ·í•œ ì ìˆ˜ â†’ êµ¬ë¶„ë ¥ ì—†ìŒ)
            if len(sorted_indices) > 3:
                gap = float(similarities[sorted_indices[0]] - similarities[sorted_indices[3]])
                if gap < EMBEDDING_GAP_THRESHOLD:
                    logger.debug(
                        f"ì„ë² ë”© ê°­ ë¶€ì¡±: top1={similarities[sorted_indices[0]]:.3f}, "
                        f"top4={similarities[sorted_indices[3]]:.3f}, gap={gap:.3f}"
                    )
                    return []

            matched = []
            for idx in top_indices:
                score = similarities[idx]
                if score >= EMBEDDING_SIMILARITY_THRESHOLD:
                    node_name = self._node_names[idx]
                    node_type = self.kg.graph.nodes[node_name].get("type", "")
                    matched.append((node_name, float(score), node_type))

            if matched:
                match_str = ", ".join(
                    f"{name}({score:.2f})" for name, score, _ in matched
                )
                logger.debug(f"ğŸ” ì„ë² ë”© ë§¤ì¹­: query='{query}' â†’ {match_str}")

            return [name for name, _, _ in matched]

        except Exception as e:
            logger.debug(f"ì„ë² ë”© ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return []

    # â”€â”€ Stage 3: LLM Fallback â”€â”€

    def _extract_by_llm(self, query: str) -> list[str]:
        """
        ê·œì¹™+ì„ë² ë”©ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ëª» ì°¾ì•˜ì„ ë•Œ LLMì—ê²Œ ì˜í•™ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìš”ì²­í•œë‹¤.
        ë™ê¸° í˜¸ì¶œ (retrieve_nodeê°€ syncì´ë¯€ë¡œ).
        """
        try:
            import httpx

            # ê·¸ë˜í”„ì— ìˆëŠ” ë…¸ë“œ ëª©ë¡ì„ ì œê³µí•˜ì—¬ hallucination ë°©ì§€
            node_list = ", ".join(list(self.kg.graph.nodes)[:50])

            prompt = (
                "ë‹¹ì‹ ì€ ì˜ë£Œ í‚¤ì›Œë“œ ì¶”ì¶œê¸°ì…ë‹ˆë‹¤.\n"
                "ì•„ë˜ ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ê±´ê°•/ì˜ë£Œ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.\n"
                "ë°˜ë“œì‹œ ì•„ë˜ í›„ë³´ ëª©ë¡ì— ìˆëŠ” ë‹¨ì–´ë§Œ ì„ íƒí•˜ì„¸ìš”.\n"
                "í•´ë‹¹ ì—†ìœ¼ë©´ 'ì—†ìŒ'ì´ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”.\n\n"
                f"[í›„ë³´ ëª©ë¡]\n{node_list}\n\n"
                f"[ì‚¬ìš©ì ë©”ì‹œì§€]\n{query}\n\n"
                "[ì¶”ì¶œëœ í‚¤ì›Œë“œ (ì‰¼í‘œ êµ¬ë¶„)]"
            )

            response = httpx.post(
                f"{settings.OLLAMA_BASE_URL}/api/generate",
                json={
                    "model": settings.OLLAMA_MODEL,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.0,
                        "num_predict": 50,  # í‚¤ì›Œë“œë§Œì´ë¯€ë¡œ ì§§ê²Œ
                    },
                },
                timeout=5.0,  # ìµœëŒ€ 5ì´ˆ ëŒ€ê¸°
            )
            response.raise_for_status()

            result = response.json().get("response", "").strip()

            # "ì—†ìŒ" ì‘ë‹µ ì²˜ë¦¬
            if "ì—†ìŒ" in result or not result:
                return []

            # <think>...</think> ë¸”ë¡ ì œê±°
            result = re.sub(r"<think>.*?</think>", "", result, flags=re.DOTALL)
            result = re.sub(r"</?think>", "", result).strip()

            # ì‰¼í‘œë¡œ ë¶„ë¦¬ í›„ ì‹¤ì œ ê·¸ë˜í”„ ë…¸ë“œì¸ì§€ ê²€ì¦
            candidates = [kw.strip() for kw in result.split(",")]
            valid_keywords = [
                kw for kw in candidates
                if kw in self.kg.graph.nodes
            ]

            if valid_keywords:
                logger.info(f"ğŸ¤– LLM í‚¤ì›Œë“œ ì¶”ì¶œ: '{query}' â†’ {valid_keywords}")

            return valid_keywords[:3]

        except Exception as e:
            logger.debug(f"LLM í‚¤ì›Œë“œ ì¶”ì¶œ ìŠ¤í‚µ: {e}")
            return []

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # í¬ë§¤íŒ…
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def _format_condition_info(self, info: dict) -> str:
        """ì§ˆí™˜ ì •ë³´ë¥¼ ì½ê¸° ì‰¬ìš´ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œë‹¤."""
        parts = [f"â–¶ {info['condition']}: {info.get('description', '')}"]

        symptoms = info.get("symptoms", [])
        if symptoms:
            symptom_names = [s["name"] for s in symptoms]
            parts.append(f"  ì¦ìƒ: {', '.join(symptom_names)}")

        treatments = info.get("treatments", [])
        if treatments:
            for t in treatments:
                parts.append(f"  ê´€ë¦¬: {t['name']} â€” {t.get('desc', '')}")

        prevention = info.get("prevention", [])
        if prevention:
            prev_names = [p["name"] for p in prevention]
            parts.append(f"  ì˜ˆë°©: {', '.join(prev_names)}")

        risk_factors = info.get("risk_factors", [])
        if risk_factors:
            risk_names = [r["name"] for r in risk_factors]
            parts.append(f"  ìœ„í—˜ìš”ì¸: {', '.join(risk_names)}")

        return "\n".join(parts)

    def _format_related_nodes(self, keyword: str, nodes: list[dict]) -> str:
        """ê´€ë ¨ ë…¸ë“œ ëª©ë¡ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œë‹¤."""
        if not nodes:
            return ""

        # íƒ€ì…ë³„ ê·¸ë£¹í•‘
        by_type = {}
        for node in nodes:
            ntype = node.get("type", "unknown")
            if ntype not in by_type:
                by_type[ntype] = []
            by_type[ntype].append(node)

        parts = [f"â–¶ '{keyword}' ê´€ë ¨ ì •ë³´:"]

        type_labels = {
            NodeType.CONDITION: "ê´€ë ¨ ì§ˆí™˜",
            NodeType.SYMPTOM: "ê´€ë ¨ ì¦ìƒ",
            NodeType.TREATMENT: "ê´€ë¦¬ ë°©ë²•",
            NodeType.LIFESTYLE: "ìƒí™œ ìŠµê´€",
            NodeType.RISK_FACTOR: "ì£¼ì˜ ì‚¬í•­",
        }

        for ntype, label in type_labels.items():
            group = by_type.get(ntype, [])
            if group:
                names = [f"{n['name']}({n.get('desc', '')})" for n in group[:3]]
                parts.append(f"  {label}: {', '.join(names)}")

        return "\n".join(parts) if len(parts) > 1 else ""


# ì‹±ê¸€í†¤
_graph_rag: Optional[GraphRAGRetriever] = None


def get_graph_rag() -> GraphRAGRetriever:
    global _graph_rag
    if _graph_rag is None:
        _graph_rag = GraphRAGRetriever()
    return _graph_rag

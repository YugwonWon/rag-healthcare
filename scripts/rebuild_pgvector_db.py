"""
Cloud SQL (pgvector) ë¬¸ì„œ ë°ì´í„° ì¬êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸
- healthcare_docs/*.txt ë° ëŒ€í™”ì˜ˆì œ(conversations/*.txt)ë¥¼ 
  LangChain + pgvector (Cloud SQL)ì— ë¡œë“œ
- ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ í›„ ìµœì‹  íŒŒì¼ë¡œ ì¬êµ¬ì¶•
"""

import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.config import settings
from app.logger import get_logger

logger = get_logger(__name__)


def rebuild_pgvector_db():
    """pgvector DBì˜ ë¬¸ì„œ ì»¬ë ‰ì…˜ì„ ì¬êµ¬ì¶•"""
    
    # 1) ì„¤ì • í™•ì¸
    db_url = settings.database_url
    if not db_url:
        logger.error("DATABASE_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    logger.info(f"DB ì—°ê²°: {db_url[:40]}...")
    logger.info(f"USE_LANGCHAIN_STORE: {settings.USE_LANGCHAIN_STORE}")
    
    # 2) LangChain ìŠ¤í† ì–´ ì´ˆê¸°í™”
    from app.langchain_store import LangChainDataStore
    store = LangChainDataStore(db_url)
    
    if not store.is_postgres_enabled:
        logger.error("PostgreSQLì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # 3) ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ
    logger.info("\nğŸ—‘ï¸  ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ ì¤‘...")
    try:
        import psycopg2
        conn = psycopg2.connect(db_url)
        cur = conn.cursor()
        
        # í˜„ì¬ ë¬¸ì„œ ìˆ˜ í™•ì¸
        cur.execute("SELECT COUNT(*) FROM langchain_pg_embedding")
        before_count = cur.fetchone()[0]
        logger.info(f"   ê¸°ì¡´ ë¬¸ì„œ ìˆ˜: {before_count}")
        
        # ê¸°ì¡´ ë¬¸ì„œ ë°ì´í„° ì‚­ì œ (ì»¬ë ‰ì…˜ ë©”íƒ€ë°ì´í„°ëŠ” ìœ ì§€)
        cur.execute("DELETE FROM langchain_pg_embedding")
        conn.commit()
        logger.info(f"   -> {before_count}ê°œ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")
        
        cur.close()
        conn.close()
    except Exception as e:
        logger.error(f"   ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
        logger.info("   ê¸°ì¡´ ë°ì´í„° ì‚­ì œë¥¼ ê±´ë„ˆë›°ê³  ì¶”ê°€ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # 4) healthcare_docs ë¡œë“œ
    docs_dir = project_root / "data" / "healthcare_docs"
    conv_dir = project_root / "data" / "conversations"
    
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_community.document_loaders import TextLoader
    from langchain_core.documents import Document
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n---\n", "\n\n", "\n", " "]
    )
    
    all_docs = []
    stats = {"healthcare_docs": 0, "conversations": 0}
    
    # healthcare_docs
    if docs_dir.exists():
        txt_files = sorted(docs_dir.glob("*.txt"))
        logger.info(f"\nğŸ“‚ healthcare_docs: {len(txt_files)}ê°œ íŒŒì¼")
        
        for txt_file in txt_files:
            try:
                loader = TextLoader(str(txt_file), encoding="utf-8")
                docs = loader.load()
                # ë©”íƒ€ë°ì´í„°ì— ì¹´í…Œê³ ë¦¬ ì¶”ê°€
                for doc in docs:
                    doc.metadata["category"] = "healthcare_docs"
                    doc.metadata["source_name"] = txt_file.stem
                
                splits = splitter.split_documents(docs)
                all_docs.extend(splits)
                stats["healthcare_docs"] += len(splits)
                logger.info(f"  âœ… {txt_file.name} -> {len(splits)}ê°œ ì²­í¬")
            except Exception as e:
                logger.error(f"  âŒ {txt_file.name}: {e}")
    
    # conversations
    if conv_dir.exists():
        conv_files = sorted(conv_dir.glob("*.txt"))
        logger.info(f"\nğŸ“‚ conversations: {len(conv_files)}ê°œ íŒŒì¼")
        
        for txt_file in conv_files:
            try:
                loader = TextLoader(str(txt_file), encoding="utf-8")
                docs = loader.load()
                for doc in docs:
                    doc.metadata["category"] = "conversations"
                    doc.metadata["source_name"] = txt_file.stem
                
                splits = splitter.split_documents(docs)
                all_docs.extend(splits)
                stats["conversations"] += len(splits)
                logger.info(f"  âœ… {txt_file.name} -> {len(splits)}ê°œ ì²­í¬")
            except Exception as e:
                logger.error(f"  âŒ {txt_file.name}: {e}")
    
    # 5) pgvectorì— ì¼ê´„ ë¡œë“œ
    total = len(all_docs)
    logger.info(f"\nğŸ“¤ pgvectorì— {total}ê°œ ì²­í¬ ë¡œë”© ì¤‘...")
    
    # ë°°ì¹˜ ì²˜ë¦¬ (í•œ ë²ˆì— ë„ˆë¬´ ë§ì´ ë³´ë‚´ë©´ íƒ€ì„ì•„ì›ƒ ê°€ëŠ¥)
    batch_size = 50
    loaded = 0
    
    for i in range(0, total, batch_size):
        batch = all_docs[i:i + batch_size]
        try:
            store.vectorstore.add_documents(batch)
            loaded += len(batch)
            logger.info(f"  [{loaded}/{total}] ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"  ë°°ì¹˜ {i//batch_size + 1} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # 6) ìµœì¢… í™•ì¸
    try:
        import psycopg2
        conn = psycopg2.connect(db_url)
        cur = conn.cursor()
        cur.execute("SELECT COUNT(*) FROM langchain_pg_embedding")
        final_count = cur.fetchone()[0]
        cur.close()
        conn.close()
    except Exception:
        final_count = "í™•ì¸ ë¶ˆê°€"
    
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“Š pgvector DB ì¬êµ¬ì¶• ê²°ê³¼:")
    logger.info(f"   healthcare_docs ì²­í¬: {stats['healthcare_docs']}ê°œ")
    logger.info(f"   conversations ì²­í¬:   {stats['conversations']}ê°œ")
    logger.info(f"   ì´ ë¡œë“œ ì²­í¬:         {loaded}ê°œ")
    logger.info(f"   DB ìµœì¢… ë¬¸ì„œ ìˆ˜:      {final_count}ê°œ")
    logger.info(f"{'='*60}")


def test_search():
    """pgvector ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    from app.langchain_store import get_langchain_store
    store = get_langchain_store()
    
    test_queries = ["í ê±´ê°• ê´€ë¦¬", "ìˆ˜ë©´ ì¥ì•  ê°œì„ ", "ê°±ë…„ê¸° ì¦ìƒ", "êµ¬ê°• ê´€ë¦¬ ë°©ë²•"]
    
    print("\n" + "=" * 60)
    print("ğŸ” pgvector ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    for query in test_queries:
        results = store.search_documents(query, k=3)
        print(f"\nğŸ” '{query}':")
        if results:
            for i, r in enumerate(results):
                source = r["metadata"].get("source_name", "unknown")
                category = r["metadata"].get("category", "unknown")
                score = r.get("score", "N/A")
                content_preview = r["content"][:120].replace("\n", " ")
                print(f"  [{i+1}] ({category}/{source}, score:{score:.3f}) {content_preview}...")
        else:
            print("  ê²°ê³¼ ì—†ìŒ")


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Cloud SQL pgvector ë¬¸ì„œ ì¬êµ¬ì¶•")
    parser.add_argument("--test", action="store_true", help="ì¬êµ¬ì¶• í›„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    args = parser.parse_args()
    
    logger.info("ğŸš€ Cloud SQL pgvector DB ì¬êµ¬ì¶• ì‹œì‘")
    rebuild_pgvector_db()
    
    if args.test:
        test_search()
    
    logger.info("âœ… ì™„ë£Œ!")

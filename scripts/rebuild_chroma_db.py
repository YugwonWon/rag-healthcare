"""
ChromaDB ë°ì´í„° ì „ì²´ ì¬êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸
- healthcare_docs/*.txt ë° ëŒ€í™”ì˜ˆì œ(conversations/*.txt)ë¥¼ ëª¨ë‘ ë¡œë“œ
- ê¸°ì¡´ docs ì»¬ë ‰ì…˜ì„ ì´ˆê¸°í™”í•œ ë’¤ ìµœì‹  íŒŒì¼ë¡œ ì¬êµ¬ì¶•
"""

import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.vector_store import get_chroma_handler
from app.logger import get_logger

logger = get_logger(__name__)


def load_text_file(file_path: Path, category: str) -> list[dict]:
    """
    í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ì„œ ì²­í¬ë¡œ ë¶„í• 
    
    Args:
        file_path: í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        category: ì¹´í…Œê³ ë¦¬ (healthcare_docs / conversations)
    
    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸ (ê° ì²­í¬ëŠ” text, metadataë¥¼ í¬í•¨)
    """
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    # êµ¬ë¶„ì„ (---) ë˜ëŠ” ë¹ˆ ì¤„ë¡œ ì„¹ì…˜ ë¶„í• 
    sections = []
    current_section = []
    
    for line in content.split("\n"):
        if line.strip().startswith("---") or (not line.strip() and current_section and len("\n".join(current_section)) > 500):
            if current_section:
                section_text = "\n".join(current_section).strip()
                if section_text:
                    sections.append(section_text)
                current_section = []
        else:
            current_section.append(line)
    
    # ë§ˆì§€ë§‰ ì„¹ì…˜ ì¶”ê°€
    if current_section:
        section_text = "\n".join(current_section).strip()
        if section_text:
            sections.append(section_text)
    
    # ì„¹ì…˜ì´ ë„ˆë¬´ í¬ë©´ ì¶”ê°€ ë¶„í•  (1000ì ê¸°ì¤€)
    chunks = []
    for section in sections:
        if len(section) > 1000:
            paragraphs = section.split("\n\n")
            current_chunk = ""
            for para in paragraphs:
                if len(current_chunk) + len(para) > 1000:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = para
                else:
                    current_chunk += "\n\n" + para if current_chunk else para
            if current_chunk:
                chunks.append(current_chunk.strip())
        else:
            chunks.append(section)
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    file_name = file_path.stem
    result = []
    for i, chunk in enumerate(chunks):
        if chunk.strip():
            result.append({
                "text": chunk,
                "metadata": {
                    "source": file_name,
                    "file_path": str(file_path),
                    "chunk_index": i,
                    "total_chunks": len(chunks),
                    "category": category
                }
            })
    
    return result


def rebuild_database(clear_existing: bool = True) -> dict:
    """
    ChromaDB docs ì»¬ë ‰ì…˜ì„ ì¬êµ¬ì¶•
    
    Args:
        clear_existing: Trueë©´ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œ í›„ ì¬êµ¬ì¶•
    
    Returns:
        ê²°ê³¼ í†µê³„
    """
    chroma = get_chroma_handler()
    
    # ê¸°ì¡´ ë¬¸ì„œ ìˆ˜ í™•ì¸
    existing_count = chroma._docs_collection.count()
    logger.info(f"ê¸°ì¡´ ë¬¸ì„œ ìˆ˜: {existing_count}")
    
    if clear_existing and existing_count > 0:
        logger.info("ê¸°ì¡´ docs ì»¬ë ‰ì…˜ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤...")
        # ê¸°ì¡´ ë¬¸ì„œ ëª¨ë‘ ê°€ì ¸ì™€ì„œ ì‚­ì œ
        existing = chroma._docs_collection.get()
        if existing and existing.get("ids"):
            chroma._docs_collection.delete(ids=existing["ids"])
            logger.info(f"  -> {len(existing['ids'])}ê°œ ê¸°ì¡´ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")
    
    stats = {"healthcare_docs": 0, "conversations": 0, "total_chunks": 0, "files_processed": 0}
    
    # 1) healthcare_docs ë¡œë“œ
    docs_dir = project_root / "data" / "healthcare_docs"
    if docs_dir.exists():
        txt_files = sorted(docs_dir.glob("*.txt"))
        logger.info(f"\nğŸ“‚ healthcare_docs: {len(txt_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        for txt_file in txt_files:
            try:
                chunks = load_text_file(txt_file, category="healthcare_docs")
                if not chunks:
                    logger.warning(f"  âš ï¸ ì²­í¬ ì—†ìŒ: {txt_file.name}")
                    continue
                
                documents = [c["text"] for c in chunks]
                metadatas = [c["metadata"] for c in chunks]
                ids = [f"hd_{txt_file.stem}_{i}" for i in range(len(chunks))]
                
                chroma.add_documents(documents=documents, metadatas=metadatas, ids=ids)
                
                stats["healthcare_docs"] += len(chunks)
                stats["files_processed"] += 1
                logger.info(f"  âœ… {txt_file.name} -> {len(chunks)}ê°œ ì²­í¬")
            except Exception as e:
                logger.error(f"  âŒ ì‹¤íŒ¨: {txt_file.name} - {e}")
    else:
        logger.warning(f"healthcare_docs í´ë” ì—†ìŒ: {docs_dir}")
    
    # 2) ëŒ€í™”ì˜ˆì œ(conversations) ë¡œë“œ
    conv_dir = project_root / "data" / "conversations"
    if conv_dir.exists():
        conv_files = sorted(conv_dir.glob("*.txt"))
        logger.info(f"\nğŸ“‚ conversations: {len(conv_files)}ê°œ íŒŒì¼ ë°œê²¬")
        
        for txt_file in conv_files:
            try:
                chunks = load_text_file(txt_file, category="conversations")
                if not chunks:
                    logger.warning(f"  âš ï¸ ì²­í¬ ì—†ìŒ: {txt_file.name}")
                    continue
                
                documents = [c["text"] for c in chunks]
                metadatas = [c["metadata"] for c in chunks]
                ids = [f"cv_{txt_file.stem}_{i}" for i in range(len(chunks))]
                
                chroma.add_documents(documents=documents, metadatas=metadatas, ids=ids)
                
                stats["conversations"] += len(chunks)
                stats["files_processed"] += 1
                logger.info(f"  âœ… {txt_file.name} -> {len(chunks)}ê°œ ì²­í¬")
            except Exception as e:
                logger.error(f"  âŒ ì‹¤íŒ¨: {txt_file.name} - {e}")
    else:
        logger.warning(f"conversations í´ë” ì—†ìŒ: {conv_dir}")
    
    stats["total_chunks"] = stats["healthcare_docs"] + stats["conversations"]
    
    # ìµœì¢… í™•ì¸
    final_count = chroma._docs_collection.count()
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“Š ì¬êµ¬ì¶• ê²°ê³¼:")
    logger.info(f"   healthcare_docs ì²­í¬: {stats['healthcare_docs']}ê°œ")
    logger.info(f"   conversations ì²­í¬:   {stats['conversations']}ê°œ")
    logger.info(f"   ì²˜ë¦¬ëœ íŒŒì¼ ìˆ˜:       {stats['files_processed']}ê°œ")
    logger.info(f"   ì´ ì²­í¬ ìˆ˜:           {stats['total_chunks']}ê°œ")
    logger.info(f"   DB ìµœì¢… ë¬¸ì„œ ìˆ˜:      {final_count}ê°œ")
    logger.info(f"{'='*60}")
    
    return stats


def test_search():
    """ì¬êµ¬ì¶• í›„ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰"""
    chroma = get_chroma_handler()
    
    test_queries = ["í ê±´ê°•", "ìˆ˜ë©´ ì¥ì• ", "ê°±ë…„ê¸° ì¦ìƒ", "êµ¬ê°• ê´€ë¦¬"]
    
    print("\n" + "=" * 60)
    print("ğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼")
    print("=" * 60)
    
    for query in test_queries:
        results = chroma.search_documents(query, n_results=2)
        print(f"\nğŸ” '{query}':")
        
        if results and results.get("documents") and results["documents"][0]:
            for i, (doc, metadata) in enumerate(zip(results["documents"][0], results["metadatas"][0])):
                source = metadata.get("source", "unknown")
                category = metadata.get("category", "unknown")
                print(f"  [{i+1}] ({category}/{source}) {doc[:150]}...")
        else:
            print("  ê²°ê³¼ ì—†ìŒ")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ChromaDB ë°ì´í„° ì¬êµ¬ì¶•")
    parser.add_argument("--no-clear", action="store_true", help="ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ì§€ ì•Šê³  ì¶”ê°€ë§Œ í•¨")
    parser.add_argument("--test", action="store_true", help="ì¬êµ¬ì¶• í›„ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤í–‰")
    args = parser.parse_args()
    
    logger.info("ğŸš€ ChromaDB ë°ì´í„° ì¬êµ¬ì¶• ì‹œì‘")
    stats = rebuild_database(clear_existing=not args.no_clear)
    
    if args.test:
        test_search()
    
    logger.info("âœ… ì™„ë£Œ!")


if __name__ == "__main__":
    main()

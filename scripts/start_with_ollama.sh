#!/bin/bash
# Cloud Runìš© ì‹œìž‘ ìŠ¤í¬ë¦½íŠ¸: Ollama + FastAPI (ë²”ìš© ëª¨ë¸ ì§€ì›)
# ëª¨ë¸ì€ Dockerfileì—ì„œ ì´ë¯¸ pre-registeredë˜ë¯€ë¡œ ëŸ°íƒ€ìž„ì—ì„œëŠ” ì„œë²„ ì‹œìž‘ë§Œ í•„ìš”

# set -e ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - ë¶€ë¶„ ì‹¤íŒ¨ì—ë„ ì„œë²„ëŠ” ì‹œìž‘í•´ì•¼ í•¨

export PYTHONIOENCODING=utf-8
export OLLAMA_DEBUG=0

# â”€â”€â”€ 1. Ollama ì„œë²„ ì‹œìž‘ â”€â”€â”€
echo "ðŸš€ Starting Ollama server..."
ollama serve > /dev/null 2>&1 &
OLLAMA_PID=$!

echo "â³ Waiting for Ollama to be ready..."
OLLAMA_READY=false
for i in $(seq 1 30); do
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "âœ… Ollama is ready! (${i}s)"
        OLLAMA_READY=true
        break
    fi
    sleep 1
done

if [ "$OLLAMA_READY" = false ]; then
    echo "âš ï¸ Ollama not ready yet, but starting server anyway..."
fi

# â”€â”€â”€ 2. ëª¨ë¸ í™•ì¸ (Dockerfileì—ì„œ ì´ë¯¸ pre-registered) â”€â”€â”€
MODEL_NAME="${OLLAMA_MODEL:-k-exaone-counseling}"
echo "ðŸ“¦ Model: ${MODEL_NAME}"

if [ "$OLLAMA_READY" = true ]; then
    if ollama list 2>/dev/null | grep -q "${MODEL_NAME}"; then
        echo "âœ… Model already registered (pre-built)!"
    else
        echo "âš ï¸ Model not found, attempting registration..."
        MODELS_DIR="/app/models"
        MODELFILE="${MODELS_DIR}/Modelfile.${MODEL_NAME}"
        GGUF_FILE="${MODELS_DIR}/${MODEL_NAME}.gguf"
        if [ -f "${MODELFILE}" ]; then
            cd "${MODELS_DIR}" && ollama create "${MODEL_NAME}" -f "Modelfile.${MODEL_NAME}" 2>&1 && cd /app || echo "âš ï¸ Model create failed, continuing..."
        elif [ -f "${GGUF_FILE}" ]; then
            printf "FROM ${GGUF_FILE}\nPARAMETER temperature 0.1\n" > /tmp/Modelfile.auto
            ollama create "${MODEL_NAME}" -f /tmp/Modelfile.auto 2>&1 || echo "âš ï¸ Model create failed, continuing..."
            rm -f /tmp/Modelfile.auto
        else
            ollama pull "${MODEL_NAME}" 2>&1 || echo "âš ï¸ Model pull failed, continuing..."
        fi
    fi
else
    echo "âš ï¸ Ollama not ready, skipping model check"
fi

# â”€â”€â”€ 3. ì„œë²„ ì‹œìž‘ í›„ ë°±ê·¸ë¼ìš´ë“œ ì›Œë°ì—… â”€â”€â”€
echo "ðŸ”¥ Warmup will run after server starts..."

# â”€â”€â”€ 4. ë””ë ‰í† ë¦¬ ì¤€ë¹„ â”€â”€â”€
mkdir -p /app/data/chroma /app/logs

# â”€â”€â”€ 5. FastAPI ì„œë²„ ì‹¤í–‰ â”€â”€â”€
PORT=${PORT:-8080}
echo "ðŸŒ Starting FastAPI server on port ${PORT}..."
echo "   Model: ${MODEL_NAME}"

# ì„œë²„ ì‹œìž‘ í›„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì›Œë°ì—… + ë°ì´í„° ì´ˆê¸°í™”
(
    sleep 10
    # ì›Œë°ì—…
    if [ "$OLLAMA_READY" = true ]; then
        curl -s http://localhost:11434/api/generate \
            -d "{\"model\": \"${MODEL_NAME}\", \"prompt\": \"hello\", \"stream\": false}" > /dev/null 2>&1 || true
        echo "âœ… Background warmup complete!"
    fi
) &

exec uvicorn app.main:app --host 0.0.0.0 --port ${PORT}

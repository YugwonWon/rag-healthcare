#!/bin/bash
# Cloud Runìš© ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸: Ollama + Qwen + FastAPI

set -e

echo "ğŸš€ Starting Ollama server..."
ollama serve &
OLLAMA_PID=$!

# Ollama ì„œë²„ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
echo "â³ Waiting for Ollama to be ready..."
for i in {1..60}; do
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "âœ… Ollama is ready!"
        break
    fi
    if [ $i -eq 60 ]; then
        echo "âŒ Ollama failed to start"
        exit 1
    fi
    sleep 1
done

# ëª¨ë¸ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
MODEL_NAME="${OLLAMA_MODEL:-qwen2.5:3b}"
echo "ğŸ“¦ Checking model: ${MODEL_NAME}..."

if ! ollama list | grep -q "${MODEL_NAME}"; then
    echo "â¬‡ï¸ Pulling model: ${MODEL_NAME} (this may take a while on first run)..."
    ollama pull ${MODEL_NAME}
    echo "âœ… Model pulled successfully!"
else
    echo "âœ… Model already available!"
fi

# ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ (ì›Œë°ì—…)
echo "ğŸ”¥ Warming up model..."
curl -s http://localhost:11434/api/generate -d "{\"model\": \"${MODEL_NAME}\", \"prompt\": \"hello\", \"stream\": false}" > /dev/null 2>&1 || true
echo "âœ… Model warmed up!"

# FastAPI ì•± ì‹¤í–‰
echo "ğŸŒ Starting FastAPI server on port ${PORT:-8000}..."
exec uvicorn app.main:app --host 0.0.0.0 --port ${PORT:-8000}

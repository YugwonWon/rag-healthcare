#!/bin/bash
# Cloud RunÏö© ÏãúÏûë Ïä§ÌÅ¨Î¶ΩÌä∏: Ollama + Qwen + FastAPI

set -e

# UTF-8 ÌôòÍ≤Ω Î∞è Ollama Î°úÍ∑∏ Î†àÎ≤® ÏÑ§Ï†ï
export PYTHONIOENCODING=utf-8
export OLLAMA_DEBUG=0

echo "üöÄ Starting Ollama server..."
ollama serve 2>&1 | grep -v "print_info\|llama_\|ggml_\|rope_\|vocab\|token" &
OLLAMA_PID=$!

# Ollama ÏÑúÎ≤ÑÍ∞Ä Ï§ÄÎπÑÎê† ÎïåÍπåÏßÄ ÎåÄÍ∏∞
echo "‚è≥ Waiting for Ollama to be ready..."
for i in {1..60}; do
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "‚úÖ Ollama is ready!"
        break
    fi
    if [ $i -eq 60 ]; then
        echo "‚ùå Ollama failed to start"
        exit 1
    fi
    sleep 1
done

# Î™®Îç∏ ÌôïÏù∏ Î∞è Îã§Ïö¥Î°úÎìú
# kanana = kakaocorp/kanana-nano-2.1b-instruct (HuggingFace GGUF: ch00n/kanana-nano-2.1b-instruct-Q4_K_M-GGUF)
MODEL_NAME="${OLLAMA_MODEL:-kanana}"
echo "üì¶ Checking model: ${MODEL_NAME}..."

if ! ollama list | grep -q "${MODEL_NAME}"; then
    # kanana Î™®Îç∏ÏùÄ HuggingFaceÏóêÏÑú GGUF Îã§Ïö¥Î°úÎìú ÌõÑ Îì±Î°ù
    if [ "${MODEL_NAME}" = "kanana" ]; then
        echo "‚¨áÔ∏è Downloading kanana-nano-2.1b-instruct from HuggingFace..."
        echo "   Source: ch00n/kanana-nano-2.1b-instruct-Q4_K_M-GGUF"
        GGUF_URL="https://huggingface.co/ch00n/kanana-nano-2.1b-instruct-Q4_K_M-GGUF/resolve/main/kanana-nano-2.1b-instruct-q4_k_m.gguf"
        GGUF_PATH="/app/models/${MODEL_NAME}.gguf"
        
        # Îã§Ïö¥Î°úÎìú (Ïû¨ÏãúÎèÑ Ìè¨Ìï®)
        for attempt in 1 2 3; do
            if curl -L --retry 3 --retry-delay 5 -o "${GGUF_PATH}" "${GGUF_URL}"; then
                echo "‚úÖ GGUF downloaded successfully!"
                break
            else
                echo "‚ö†Ô∏è Download attempt $attempt failed, retrying..."
                sleep 5
            fi
            if [ $attempt -eq 3 ]; then
                echo "‚ùå Failed to download GGUF after 3 attempts"
                exit 1
            fi
        done
        
        # Modelfile ÏÉùÏÑ± Î∞è Îì±Î°ù
        cat > /tmp/Modelfile.${MODEL_NAME} << EOF
FROM ${GGUF_PATH}
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_predict 512
SYSTEM "ÎãπÏã†ÏùÄ ÏπòÎß§ÎÖ∏Ïù∏ÏùÑ ÎèåÎ≥¥Îäî Îî∞ÎúªÌïòÍ≥† ÏπúÏ†àÌïú AI ÎèÑÏö∞ÎØ∏ÏûÖÎãàÎã§. Î∞òÎìúÏãú ÌïúÍµ≠Ïñ¥Î°úÎßå ÏùëÎãµÌïòÏÑ∏Ïöî."
EOF
        echo "üìù Registering ${MODEL_NAME} model with Ollama..."
        ollama create ${MODEL_NAME} -f /tmp/Modelfile.${MODEL_NAME}
        echo "‚úÖ ${MODEL_NAME} model registered!"
    else
        # ÏùºÎ∞ò Ollama Î™®Îç∏ pull
        echo "‚¨áÔ∏è Pulling model: ${MODEL_NAME} (this may take a while on first run)..."
        echo "   Progress logs suppressed. Please wait..."
        # 3Î≤à Ïû¨ÏãúÎèÑ (ÏßÑÌñâ ÏÉÅÌô© Î°úÍ∑∏ Ïà®ÍπÄ)
        for attempt in 1 2 3; do
            if ollama pull ${MODEL_NAME} 2>&1 | grep -E "(success|error|failed|pulling [a-f0-9]+:.*100%)" || [ ${PIPESTATUS[0]} -eq 0 ]; then
                echo "‚úÖ Model pulled successfully!"
                break
            else
                echo "‚ö†Ô∏è Pull attempt $attempt failed, retrying..."
                sleep 5
            fi
            if [ $attempt -eq 3 ]; then
                echo "‚ùå Failed to pull model after 3 attempts"
                exit 1
            fi
        done
    fi
else
    echo "‚úÖ Model already available!"
fi

# Î™®Îç∏ Í≤ÄÏ¶ù (ÌïúÍ∏Ä ÌÖåÏä§Ìä∏) - Î°úÍ∑∏ Í∞ÑÏÜåÌôî
echo "üîç Verifying model..."
KOREAN_TEST=$(curl -s http://localhost:11434/api/generate -d "{\"model\": \"${MODEL_NAME}\", \"prompt\": \"ÏïàÎÖïÌïòÏÑ∏ÏöîÎùºÍ≥† ÎßêÌï¥Ï£ºÏÑ∏Ïöî\", \"stream\": false}" 2>&1)
if echo "$KOREAN_TEST" | grep -q "ÏïàÎÖï"; then
    echo "‚úÖ Korean language support verified!"
else
    echo "‚ö†Ô∏è Model may have issues with Korean, but continuing..."
fi

# Î™®Îç∏ ÎØ∏Î¶¨ Î°úÎìú (ÏõåÎ∞çÏóÖ) - Î°úÍ∑∏ Í∞ÑÏÜåÌôî
echo "üî• Warming up model..."
curl -s http://localhost:11434/api/generate -d "{\"model\": \"${MODEL_NAME}\", \"prompt\": \"hello\", \"stream\": false}" > /dev/null 2>&1 || true
echo "‚úÖ Model ready!"

# ChromaDB ÏÉÅÌÉú ÌôïÏù∏ Î∞è Î¨∏ÏÑú Ï¥àÍ∏∞Ìôî
echo "üìö Checking ChromaDB data..."
if [ -d "/app/data/chroma" ]; then
    echo "‚úÖ ChromaDB directory exists"
    ls -la /app/data/chroma/ || true
else
    echo "‚ö†Ô∏è ChromaDB directory not found, creating..."
    mkdir -p /app/data/chroma
fi

# Î¨∏ÏÑú Ïàò ÌôïÏù∏ Î∞è Ï¥àÍ∏∞Ìôî (PythonÏúºÎ°ú)
# ÏÉà Î¨∏ÏÑúÍ∞Ä Ï∂îÍ∞ÄÎêú Í≤ΩÏö∞ÏóêÎèÑ ÏûêÎèôÏúºÎ°ú Î°úÎìú
echo "üìÑ Checking and loading documents..."
python3 -c "
from pathlib import Path
from app.vector_store import get_chroma_handler
chroma = get_chroma_handler()
stats = chroma.get_collection_stats()
print(f'Current - Documents: {stats[\"documents\"]}')
print(f'          Conversations: {stats[\"conversations\"]}')
print(f'          Profiles: {stats[\"patient_profiles\"]}')

# Ìï≠ÏÉÅ healthcare_docs Ìè¥ÎçîÏùò Î¨∏ÏÑúÎ•º ÌôïÏù∏ÌïòÍ≥† ÏÉà Î¨∏ÏÑúÍ∞Ä ÏûàÏúºÎ©¥ Î°úÎìú
import sys
sys.path.insert(0, '/app')
docs_dir = Path('/app/data/healthcare_docs')

if docs_dir.exists():
    # Ìè¥Îçî ÎÇ¥ Î¨∏ÏÑú ÌååÏùº Ïàò ÌôïÏù∏
    doc_files = list(docs_dir.glob('*.txt')) + list(docs_dir.glob('*.md'))
    print(f'üìÅ Found {len(doc_files)} document files in healthcare_docs/')
    
    if len(doc_files) > stats['documents']:
        print('‚¨ÜÔ∏è New documents detected, reloading all documents...')
        from scripts.load_healthcare_docs import load_all_documents
        load_all_documents(docs_dir)
        stats = chroma.get_collection_stats()
        print(f'After loading - Documents: {stats[\"documents\"]}')
    elif stats['documents'] == 0:
        print('‚ö†Ô∏è No documents in DB, loading healthcare docs...')
        from scripts.load_healthcare_docs import load_all_documents
        load_all_documents(docs_dir)
        stats = chroma.get_collection_stats()
        print(f'After loading - Documents: {stats[\"documents\"]}')
    else:
        print('‚úÖ Documents already up to date')
else:
    print(f'‚ö†Ô∏è Healthcare docs directory not found: {docs_dir}')
"

# FastAPI Ïï± Ïã§Ìñâ
echo "üåê Starting FastAPI server on port ${PORT:-8000}..."
exec uvicorn app.main:app --host 0.0.0.0 --port ${PORT:-8000}

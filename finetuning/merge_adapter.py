"""
LoRA ì–´ëŒ‘í„° ë³‘í•© ìŠ¤í¬ë¦½íŠ¸
íŒŒì¸íŠœë‹ëœ LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•©
"""

import argparse
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


def merge_lora_adapter(
    base_model_path: str,
    adapter_path: str,
    output_path: str,
    push_to_hub: bool = False,
    hub_model_id: str = None,
):
    """
    LoRA ì–´ëŒ‘í„°ì™€ ë² ì´ìŠ¤ ëª¨ë¸ ë³‘í•©
    
    Args:
        base_model_path: ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ
        adapter_path: LoRA ì–´ëŒ‘í„° ê²½ë¡œ
        output_path: ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        push_to_hub: HuggingFace Hubì— ì—…ë¡œë“œ ì—¬ë¶€
        hub_model_id: Hub ëª¨ë¸ ID
    """
    print(f"ğŸ”„ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì¤‘: {base_model_path}")
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
    )
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        trust_remote_code=True,
    )
    
    print(f"ğŸ”„ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì¤‘: {adapter_path}")
    
    # LoRA ì–´ëŒ‘í„° ì ìš©
    model = PeftModel.from_pretrained(model, adapter_path)
    
    print("ğŸ”„ ëª¨ë¸ ë³‘í•© ì¤‘...")
    
    # ì–´ëŒ‘í„° ë³‘í•©
    model = model.merge_and_unload()
    
    print(f"ğŸ’¾ ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ ì¤‘: {output_path}")
    
    # ë³‘í•©ëœ ëª¨ë¸ ì €ì¥
    model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)
    
    # Hubì— ì—…ë¡œë“œ
    if push_to_hub and hub_model_id:
        print(f"â˜ï¸ Hubì— ì—…ë¡œë“œ ì¤‘: {hub_model_id}")
        model.push_to_hub(hub_model_id)
        tokenizer.push_to_hub(hub_model_id)
    
    print("âœ… ë³‘í•© ì™„ë£Œ!")


def create_ollama_modelfile(
    model_path: str,
    output_path: str,
    model_name: str = "qwen-healthcare"
):
    """
    Ollama Modelfile ìƒì„±
    
    Args:
        model_path: GGUF ëª¨ë¸ ê²½ë¡œ
        output_path: Modelfile ì €ì¥ ê²½ë¡œ
        model_name: ëª¨ë¸ ì´ë¦„
    """
    modelfile_content = f'''# Qwen 2.5 Healthcare ì¹˜ë§¤ì¼€ì–´ ëª¨ë¸
FROM {model_path}

# íŒŒë¼ë¯¸í„° ì„¤ì •
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_predict 2048

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM """ë‹¹ì‹ ì€ ì¹˜ë§¤ë…¸ì¸ì„ ëŒë³´ëŠ” ë”°ëœ»í•˜ê³  ì¹œì ˆí•œ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ ëŒ€í™”í•´ì£¼ì„¸ìš”:

1. í•­ìƒ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•˜ê³ , ì²œì²œíˆ ëª…í™•í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
2. ë³µì¡í•œ ë‚´ìš©ì€ ì§§ê³  ê°„ë‹¨í•œ ë¬¸ì¥ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì „ë‹¬í•©ë‹ˆë‹¤.
3. í™˜ìì˜ ê°ì •ì„ ì¡´ì¤‘í•˜ê³  ê³µê°í•˜ë©° ëŒ€í™”í•©ë‹ˆë‹¤.
4. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ì—¬ ì—°ì†ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.
5. ë³µì•½ ì‹œê°„, ì‹ì‚¬, ì‚°ì±… ë“± ì¼ìƒ ë£¨í‹´ì„ ë¶€ë“œëŸ½ê²Œ ìƒê¸°ì‹œí‚µë‹ˆë‹¤.
6. ìœ„í—˜í•œ ìƒí™©ì´ë‚˜ ê±´ê°• ì´ìƒ ì§•í›„ê°€ ê°ì§€ë˜ë©´ ë³´í˜¸ì/ì˜ë£Œì§„ ì—°ë½ì„ ê¶Œí•©ë‹ˆë‹¤.
"""

# í…œí”Œë¦¿ (Qwen í˜•ì‹)
TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
{{ .Response }}<|im_end|>
"""
'''
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(modelfile_content)
    
    print(f"âœ… Modelfile ìƒì„±ë¨: {output_path}")
    print(f"ğŸ“ Ollamaì— ë“±ë¡í•˜ë ¤ë©´:")
    print(f"   ollama create {model_name} -f {output_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="LoRA ì–´ëŒ‘í„° ë³‘í•©")
    
    subparsers = parser.add_subparsers(dest="command")
    
    # merge ëª…ë ¹
    merge_parser = subparsers.add_parser("merge", help="LoRA ì–´ëŒ‘í„° ë³‘í•©")
    merge_parser.add_argument("--base_model", type=str, required=True, help="ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ")
    merge_parser.add_argument("--adapter", type=str, required=True, help="LoRA ì–´ëŒ‘í„° ê²½ë¡œ")
    merge_parser.add_argument("--output", type=str, required=True, help="ì¶œë ¥ ê²½ë¡œ")
    merge_parser.add_argument("--push_to_hub", action="store_true", help="Hubì— ì—…ë¡œë“œ")
    merge_parser.add_argument("--hub_model_id", type=str, help="Hub ëª¨ë¸ ID")
    
    # modelfile ëª…ë ¹
    modelfile_parser = subparsers.add_parser("modelfile", help="Ollama Modelfile ìƒì„±")
    modelfile_parser.add_argument("--model_path", type=str, required=True, help="GGUF ëª¨ë¸ ê²½ë¡œ")
    modelfile_parser.add_argument("--output", type=str, default="Modelfile", help="Modelfile ê²½ë¡œ")
    modelfile_parser.add_argument("--name", type=str, default="qwen-healthcare", help="ëª¨ë¸ ì´ë¦„")
    
    args = parser.parse_args()
    
    if args.command == "merge":
        merge_lora_adapter(
            base_model_path=args.base_model,
            adapter_path=args.adapter,
            output_path=args.output,
            push_to_hub=args.push_to_hub,
            hub_model_id=args.hub_model_id,
        )
    elif args.command == "modelfile":
        create_ollama_modelfile(
            model_path=args.model_path,
            output_path=args.output,
            model_name=args.name,
        )
    else:
        parser.print_help()

"""
í†µí•© ëŒ€í™” ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ (v2)
- conversations/*.txt â†’ íŒŒì¸íŠœë‹ìš© JSONL
- ë‹¤ì–‘í•œ ì–‘ì‹ í†µí•© ì²˜ë¦¬ (NFD íŒŒì¼ëª… í¬í•¨)
- ë°ì´í„° ì¦ê°•: ìŠ¬ë¼ì´ë”© ìœˆë„ìš°, ë™ì¼ ì£¼ì œ í•©ì„± ë©€í‹°í„´
- ì¶œë ¥: train_counseling.jsonl / val_counseling.jsonl (Kanana í•™ìŠµìš©)

ì‚¬ìš©ë²•:
    python prepare_finetuning_data.py
    python prepare_finetuning_data.py --augment --max_turns 4
"""

import os
import re
import json
import random
import unicodedata
from pathlib import Path
from typing import List, Dict, Tuple
from collections import defaultdict

# ==========================================
# ì„¤ì •
# ==========================================
CONVERSATIONS_DIR = Path(__file__).parent.parent / "data" / "conversations"
OUTPUT_DIR = Path(__file__).parent / "data"
TRAIN_RATIO = 0.85

# ì—­í•  ë§¤í•‘
USER_ROLES = ["ê³ ë ¹ì", "ì–´ë¥´ì‹ ", "ì´ìš©ì", "user", "user "]
ASSISTANT_ROLES = ["ìƒë‹´ì‚¬", "ê´€ë¦¬ì‚¬", "ê±´ê°•ê´€ë¦¬ì‚¬", "agent", "assistant"]

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í•™ìŠµì— ì‚¬ìš©ë  í†µì¼ëœ í”„ë¡¬í”„íŠ¸)
SYSTEM_PROMPT = "ë„ˆëŠ” ë…¸ì¸ê±´ê°•ì „ë¬¸ìƒë‹´ì‚¬ë¡œì„œ ì–´ë¥´ì‹ ì˜ ê±´ê°• ê³ ë¯¼ì— ê³µê°í•˜ë©° ì¼ìƒì—ì„œ ì‹¤ì²œí•  ìˆ˜ ìˆëŠ” ê±´ê°• ìŠµê´€ì„ ì•Œë ¤ì£¼ê³ , ì¦ìƒì´ ì‹¬ê°í•œ ê²½ìš° ì˜ì‚¬ ì§„ë£Œë¥¼ ê¶Œìœ í•œë‹¤."


# ==========================================
# í…ìŠ¤íŠ¸ ì •ë¦¬
# ==========================================

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬"""
    if not text:
        return ""
    text = re.sub(r'[á„€-á…Ÿá… -á†¿ã„±-ã…ã…-ã…£]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'\[ê²€ìƒ‰[^\]]*\]', '', text)
    text = re.sub(r'\[ë°ì´í„°ë² ì´ìŠ¤ê²€ìƒ‰\]', '', text)
    text = re.sub(r'\[ê²€ìƒ‰í•´ì„œ[^\]]*\]', '', text)
    text = re.sub(r'\[ì‘ë‹µì— ë”°ë¼ì„œ[^\]]*\]', '', text)
    text = re.sub(r'\[ì•½ ì¢…ë¥˜[^\]]*\]', '', text)
    text = re.sub(r'\?\?+', '', text)
    return text.strip()


def normalize_role(role: str) -> str:
    """ì—­í•  í‘œì¤€í™”"""
    role_clean = role.strip().lower()
    # ìƒë‹´ì‚¬(ì ê·¹ì  ì‹œìˆ ê¶Œìœ ) ê°™ì€ ë³€í˜• ì²˜ë¦¬
    role_clean = re.sub(r'\([^)]*\)', '', role_clean).strip()

    for r in USER_ROLES:
        if r.lower() in role_clean:
            return "user"
    for r in ASSISTANT_ROLES:
        if r.lower() in role_clean:
            return "assistant"
    if "system" in role_clean:
        return "system"
    return role_clean


# ==========================================
# íŒŒì‹± ë¡œì§
# ==========================================

def extract_turns(text: str) -> List[Dict]:
    """í…ìŠ¤íŠ¸ì—ì„œ ëŒ€í™” í„´ ì¶”ì¶œ (ëª¨ë“  ì–‘ì‹ í†µí•©)"""
    turns = []

    # ì—­í•  íŒ¨í„´ (ë¶ˆë¦¿ â€¢, ë‹¤ì–‘í•œ ì—­í• ëª…, ê´„í˜¸ íƒœë„ í‘œì‹œ í¬í•¨)
    role_pattern = (
        r'(?:^|\n)[â€¢Â·]?\s*'
        r'(System|system|ê³ ë ¹ì|ì–´ë¥´ì‹ |ì´ìš©ì|ìƒë‹´ì‚¬|ê´€ë¦¬ì‚¬|ê±´ê°•ê´€ë¦¬ì‚¬|'
        r'User|USer|user|Agent|agent|Assistant|assistant)'
        r'\s*(?:\([^)]*\))?'  # ì„ íƒì  ê´„í˜¸ (í˜ë¥´ì†Œë‚˜ ì •ì˜), (ì ê·¹ì  ì‹œìˆ ê¶Œìœ ) ë“±
        r'\s*[:ï¼š]\s*'
    )

    parts = re.split(role_pattern, text, flags=re.MULTILINE | re.IGNORECASE)

    for i in range(1, len(parts), 2):
        if i + 1 < len(parts):
            role = parts[i].strip()
            message = parts[i + 1].strip()
            # ë‹¤ìŒ ì„¹ì…˜ ë§ˆì»¤ ì „ê¹Œì§€ë§Œ
            message = re.split(r'(?=<[^>]+>|\[ì‚¬ë¡€|\[ìƒí™©)', message)[0].strip()
            message = clean_text(message)

            if message and len(message) > 2:
                normalized = normalize_role(role)
                if normalized in ("user", "assistant"):
                    turns.append({"role": normalized, "content": message})

    return turns


def parse_file(filepath: Path) -> List[Dict]:
    """
    ëŒ€í™” íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ê°œë³„ ëŒ€í™” ëª©ë¡ ë°˜í™˜
    ê° ëŒ€í™”: {"topic": str, "turns": [{"role", "content"}, ...]}
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    filename = unicodedata.normalize('NFC', filepath.stem)
    conversations = []

    # === ì„¹ì…˜ ë¶„ë¦¬ ì „ëµ ===
    # 1) <ì œëª©> íŒ¨í„´
    # 2) [ì‚¬ë¡€ N] / [ìƒí™© N] íŒ¨í„´
    # 3) ì‚¬ë¡€ N: íŒ¨í„´ (ì†ë°œì €ë¦¼)
    # 4) System í”„ë¡¬í”„íŠ¸ë¡œ ë¶„ë¦¬

    # <ì œëª©> ë¶„ë¦¬ ì‹œë„
    angle_sections = re.split(r'(<[^>]+>)', content)
    if len(angle_sections) > 2:
        return _parse_angle_bracket_format(angle_sections, filename)

    # [ì‚¬ë¡€/ìƒí™©] ë¶„ë¦¬ ì‹œë„
    bracket_match = re.split(r'(\[(?:ì‚¬ë¡€|ìƒí™©)\s*\d+[^\]]*\])', content)
    if len(bracket_match) > 2:
        return _parse_bracket_format(bracket_match, filename)

    # ì‚¬ë¡€ N: ë¶„ë¦¬ ì‹œë„ (ì†ë°œì €ë¦¼)
    case_match = re.split(r'(ì‚¬ë¡€\s*\d+\s*[:ï¼š])', content)
    if len(case_match) > 2:
        return _parse_case_format(case_match, filename)

    # System í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬
    system_match = re.split(
        r'(System\s*(?:\([^)]*\))?\s*[:ï¼š])',
        content, flags=re.IGNORECASE
    )
    if len(system_match) > 2:
        return _parse_system_format(system_match, filename)

    # í´ë°±: ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ëŒ€í™”ë¡œ
    turns = extract_turns(content)
    if turns:
        cleaned = _clean_turns(turns)
        if cleaned:
            conversations.append({"topic": filename, "turns": cleaned})

    return conversations


def _parse_angle_bracket_format(sections, filename) -> List[Dict]:
    """<ì œëª©> ì–‘ì‹ íŒŒì‹±"""
    conversations = []
    current_topic = filename
    current_text = ""

    for part in sections:
        part = part.strip()
        if not part:
            continue

        if re.match(r'<[^>]+>', part):
            # ì´ì „ ì„¹ì…˜ ì²˜ë¦¬
            if current_text:
                turns = extract_turns(current_text)
                cleaned = _clean_turns(turns)
                if cleaned:
                    conversations.append({"topic": current_topic, "turns": cleaned})
            current_topic = re.sub(r'[<>]', '', part).strip()
            current_text = ""
        else:
            current_text += " " + part

    # ë§ˆì§€ë§‰ ì„¹ì…˜
    if current_text:
        turns = extract_turns(current_text)
        cleaned = _clean_turns(turns)
        if cleaned:
            conversations.append({"topic": current_topic, "turns": cleaned})

    return conversations


def _parse_bracket_format(sections, filename) -> List[Dict]:
    """[ì‚¬ë¡€ N] / [ìƒí™© N] ì–‘ì‹ íŒŒì‹±"""
    conversations = []
    current_topic = filename
    current_text = ""

    for part in sections:
        part = part.strip()
        if not part:
            continue

        if re.match(r'\[(?:ì‚¬ë¡€|ìƒí™©)', part):
            if current_text:
                turns = extract_turns(current_text)
                cleaned = _clean_turns(turns)
                if cleaned:
                    conversations.append({"topic": current_topic, "turns": cleaned})
            current_topic = re.sub(r'[\[\]]', '', part).strip()
            current_text = ""
        else:
            current_text += " " + part

    if current_text:
        turns = extract_turns(current_text)
        cleaned = _clean_turns(turns)
        if cleaned:
            conversations.append({"topic": current_topic, "turns": cleaned})

    return conversations


def _parse_case_format(sections, filename) -> List[Dict]:
    """ì‚¬ë¡€ N: ì–‘ì‹ íŒŒì‹± (ë¶ˆë¦¿ í˜•ì‹)"""
    conversations = []
    current_topic = filename
    current_text = ""

    for part in sections:
        part = part.strip()
        if not part:
            continue

        if re.match(r'ì‚¬ë¡€\s*\d+', part):
            if current_text:
                turns = extract_turns(current_text)
                cleaned = _clean_turns(turns)
                if cleaned:
                    conversations.append({"topic": current_topic, "turns": cleaned})
            current_topic = part.rstrip(':ï¼š').strip()
            current_text = ""
        else:
            current_text += " " + part

    if current_text:
        turns = extract_turns(current_text)
        cleaned = _clean_turns(turns)
        if cleaned:
            conversations.append({"topic": current_topic, "turns": cleaned})

    return conversations


def _parse_system_format(sections, filename) -> List[Dict]:
    """System í”„ë¡¬í”„íŠ¸ë¡œ ë¶„ë¦¬í•˜ëŠ” ì–‘ì‹"""
    conversations = []
    current_text = ""

    for part in sections:
        part = part.strip()
        if not part:
            continue

        if re.match(r'System', part, re.IGNORECASE):
            if current_text:
                turns = extract_turns(current_text)
                cleaned = _clean_turns(turns)
                if cleaned:
                    conversations.append({"topic": filename, "turns": cleaned})
            current_text = ""
        else:
            current_text += " " + part

    if current_text:
        turns = extract_turns(current_text)
        cleaned = _clean_turns(turns)
        if cleaned:
            conversations.append({"topic": filename, "turns": cleaned})

    return conversations


def _clean_turns(turns: List[Dict]) -> List[Dict]:
    """ëŒ€í™” í„´ ì •ë¦¬: ì—°ì† ë™ì¼ ì—­í•  ë³‘í•©, user ì‹œì‘ ë³´ì¥"""
    if not turns:
        return []

    cleaned = []
    prev_role = None

    for turn in turns:
        role = turn["role"]
        content = turn["content"]

        if not content or len(content) < 3:
            continue

        if role == prev_role and cleaned:
            cleaned[-1]["content"] += " " + content
        else:
            cleaned.append({"role": role, "content": content})
            prev_role = role

    # userë¡œ ì‹œì‘í•˜ë„ë¡
    while cleaned and cleaned[0]["role"] != "user":
        cleaned.pop(0)

    # ìµœì†Œ user + assistant 1ìŒ
    if len(cleaned) < 2:
        return []

    has_user = any(t["role"] == "user" for t in cleaned)
    has_asst = any(t["role"] == "assistant" for t in cleaned)
    if not (has_user and has_asst):
        return []

    return cleaned


# ==========================================
# ë°ì´í„° ì¦ê°•
# ==========================================

def augment_sliding_window(conversations: List[Dict], window_size: int = 4) -> List[Dict]:
    """
    ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì¦ê°•
    - ì›ë³¸ ìœ ì§€ + ì„œë¸Œ ì‹œí€€ìŠ¤ ìƒì„±
    """
    augmented = []

    for conv in conversations:
        turns = conv["turns"]
        topic = conv.get("topic", "")

        # ì›ë³¸ í•­ìƒ í¬í•¨
        augmented.append(conv)

        # user-assistant ìŒ ì¶”ì¶œ
        pairs = []
        for i in range(0, len(turns) - 1, 2):
            if turns[i]["role"] == "user" and i + 1 < len(turns) and turns[i + 1]["role"] == "assistant":
                pairs.append((turns[i], turns[i + 1]))

        # 3ìŒ ì´ìƒì´ë©´ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
        if len(pairs) >= 3:
            for start in range(1, len(pairs) - 1):
                end = min(start + window_size, len(pairs))
                if end - start >= 2:
                    sub_turns = []
                    for u, a in pairs[start:end]:
                        sub_turns.extend([u, a])
                    augmented.append({"topic": topic, "turns": sub_turns})

    return augmented


def augment_combine_singles(conversations: List[Dict], max_combine: int = 3) -> List[Dict]:
    """
    ê°™ì€ ì£¼ì œ(íŒŒì¼)ì˜ ë‹¨ì¼í„´ ëŒ€í™”ë“¤ì„ ëª¨ì•„ í•©ì„± ë©€í‹°í„´ ëŒ€í™” ìƒì„±
    """
    # íŒŒì¼ëª…(ì£¼ì œ) ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì¼í„´ ëŒ€í™” ê·¸ë£¹í™”
    topic_singles = defaultdict(list)

    for conv in conversations:
        turns = conv["turns"]
        if len(turns) == 2:  # user + assistant 1ìŒ
            # íŒŒì¼ëª…ì—ì„œ ì£¼ì œ ì¶”ì¶œ
            topic_key = conv.get("topic", "").split("_")[0].split(" ")[0]
            topic_singles[topic_key].append(conv)

    combined = []
    for topic, singles in topic_singles.items():
        if len(singles) < 2:
            continue

        # 2~max_combineê°œì”© ë¬¶ì–´ì„œ í•©ì„± ë©€í‹°í„´ ìƒì„±
        random.shuffle(singles)
        for i in range(0, len(singles) - 1, max_combine):
            group = singles[i:i + max_combine]
            if len(group) >= 2:
                merged_turns = []
                for conv in group:
                    merged_turns.extend(conv["turns"])
                combined.append({"topic": topic + "_í•©ì„±", "turns": merged_turns})

    return combined


# ==========================================
# ì¶œë ¥ ë³€í™˜
# ==========================================

def to_chat_format(conv: Dict) -> Dict:
    """ChatML í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (system + user/assistant í„´)"""
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]

    for turn in conv.get("turns", []):
        content = clean_text(turn["content"])
        if content and len(content) > 2:
            messages.append({"role": turn["role"], "content": content})

    return {"messages": messages}


def validate(conv: Dict) -> bool:
    """ìœ íš¨ì„± ê²€ì¦"""
    msgs = conv.get("messages", [])
    if len(msgs) < 3:  # system + user + assistant
        return False
    has_user = any(m["role"] == "user" for m in msgs)
    has_asst = any(m["role"] == "assistant" for m in msgs)
    return has_user and has_asst


# ==========================================
# ë©”ì¸
# ==========================================

def main():
    import argparse

    parser = argparse.ArgumentParser(description="íŒŒì¸íŠœë‹ ë°ì´í„° ì „ì²˜ë¦¬ (v2)")
    parser.add_argument("--augment", action="store_true", help="ë°ì´í„° ì¦ê°• í™œì„±í™”")
    parser.add_argument("--max_turns", type=int, default=4, help="ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í¬ê¸°")
    parser.add_argument("--combine", type=int, default=3, help="ë‹¨ì¼í„´ í•©ì„± ì‹œ ìµœëŒ€ ë¬¶ìŒ ìˆ˜")
    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ“Š íŒŒì¸íŠœë‹ ë°ì´í„° ì „ì²˜ë¦¬ v2")
    print("=" * 60)

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    all_conversations = []
    file_stats = {}

    # ëª¨ë“  ëŒ€í™” íŒŒì¼ ì²˜ë¦¬
    for filepath in sorted(CONVERSATIONS_DIR.iterdir()):
        if not filepath.suffix == '.txt':
            continue

        display_name = unicodedata.normalize('NFC', filepath.name)
        print(f"\nğŸ“„ {display_name}")

        try:
            convs = parse_file(filepath)
            turn_counts = [len(c["turns"]) for c in convs]
            multi = sum(1 for t in turn_counts if t > 2)
            single = sum(1 for t in turn_counts if t == 2)

            print(f"   íŒŒì‹±: {len(convs)}ê°œ (ë©€í‹°í„´: {multi}, ì‹±ê¸€í„´: {single})")

            file_stats[display_name] = {
                "total": len(convs),
                "multi": multi,
                "single": single
            }

            all_conversations.extend(convs)

        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

    print(f"\n{'='*60}")
    print(f"ì´ íŒŒì‹±ëœ ëŒ€í™”: {len(all_conversations)}ê°œ")

    # ë°ì´í„° ì¦ê°•
    if args.augment:
        print(f"\nğŸ”„ ë°ì´í„° ì¦ê°• ì¤‘...")
        before = len(all_conversations)

        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
        augmented = augment_sliding_window(all_conversations, args.max_turns)
        print(f"   ìŠ¬ë¼ì´ë”© ìœˆë„ìš°: {before} â†’ {len(augmented)}")

        # ë‹¨ì¼í„´ í•©ì„±
        combined = augment_combine_singles(all_conversations, args.combine)
        augmented.extend(combined)
        print(f"   ë‹¨ì¼í„´ í•©ì„±: +{len(combined)}ê°œ")

        all_conversations = augmented
        print(f"   ì¦ê°• í›„ ì´: {len(all_conversations)}ê°œ")

    # ChatML í˜•ì‹ ë³€í™˜ + ìœ íš¨ì„± ê²€ì¦
    dataset = []
    for conv in all_conversations:
        chat = to_chat_format(conv)
        if validate(chat):
            dataset.append(chat)

    print(f"ìœ íš¨í•œ í•™ìŠµ ë°ì´í„°: {len(dataset)}ê°œ")

    # í†µê³„
    turn_dist = defaultdict(int)
    total_tokens = 0
    for d in dataset:
        non_sys = [m for m in d["messages"] if m["role"] != "system"]
        turn_dist[len(non_sys)] += 1
        total_tokens += sum(len(m["content"]) * 2.5 for m in d["messages"])

    print(f"\nğŸ“Š í„´ ìˆ˜ ë¶„í¬:")
    for t in sorted(turn_dist.keys()):
        print(f"   {t}í„´: {turn_dist[t]}ê°œ")
    print(f"   ëŒ€ëµì  ì´ í† í°: {int(total_tokens):,}")

    # ì…”í”Œ + ë¶„í• 
    random.seed(42)
    random.shuffle(dataset)

    split_idx = int(len(dataset) * TRAIN_RATIO)
    train_data = dataset[:split_idx]
    val_data = dataset[split_idx:]

    print(f"\nğŸ“Š ë°ì´í„°ì…‹ ë¶„í• :")
    print(f"   Train: {len(train_data)}ê°œ")
    print(f"   Valid: {len(val_data)}ê°œ")

    # ì €ì¥ (train_kanana_lora.pyê°€ ì½ëŠ” ê²½ë¡œì™€ í†µì¼)
    train_path = OUTPUT_DIR / "train_counseling.jsonl"
    val_path = OUTPUT_DIR / "val_counseling.jsonl"

    for path, data in [(train_path, train_data), (val_path, val_data)]:
        with open(path, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

    # healthcare_conversations ê²½ë¡œì—ë„ ì €ì¥ (í˜¸í™˜ì„±)
    hc_train_path = OUTPUT_DIR / "healthcare_conversations_train.jsonl"
    hc_val_path = OUTPUT_DIR / "healthcare_conversations_valid.jsonl"
    for path, data in [(hc_train_path, train_data), (hc_val_path, val_data)]:
        with open(path, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print(f"\nâœ… ì €ì¥ ì™„ë£Œ:")
    print(f"   {train_path}")
    print(f"   {val_path}")

    # ìƒ˜í”Œ ì¶œë ¥
    print(f"\n{'='*60}")
    print("ğŸ“ ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 3ê°œ):")
    print("="*60)
    for idx, sample in enumerate(train_data[:3]):
        print(f"\n--- ìƒ˜í”Œ {idx+1} (í„´: {len(sample['messages'])-1}) ---")
        for msg in sample["messages"]:
            role = msg["role"].upper()
            content = msg["content"][:80] + "..." if len(msg["content"]) > 80 else msg["content"]
            print(f"  [{role}] {content}")

    # íŒŒì¼ë³„ í†µê³„
    print(f"\n{'='*60}")
    print("íŒŒì¼ë³„ í†µê³„:")
    print("="*60)
    for fname, stats in sorted(file_stats.items()):
        print(f"  {fname}: {stats['total']}ê°œ (ë©€í‹°:{stats['multi']}, ì‹±ê¸€:{stats['single']})")


if __name__ == "__main__":
    main()

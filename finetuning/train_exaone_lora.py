"""
EXAONE 4.0 1.2B LoRA íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
ì˜¨ë””ë°”ì´ìŠ¤ ê²½ëŸ‰ ëª¨ë¸ (1.28B íŒŒë¼ë¯¸í„°)
- QLoRA 4bit: RTX 2080 Ti 11GBì—ì„œ í•™ìŠµ ê°€ëŠ¥
- EXAONE í”„ë¡¬í”„íŠ¸ í˜•ì‹: [|system|]...[|endofturn|]

ì‚¬ìš©ë²•:
    # GPU QLoRA (ê¶Œì¥)
    python train_exaone_lora.py --use_4bit --epochs 7
    
    # CPU ëª¨ë“œ (í…ŒìŠ¤íŠ¸ìš©)
    python train_exaone_lora.py --cpu --epochs 1
"""

import os
import sys

# CPU ëª¨ë“œ ì²´í¬
if "--cpu" in sys.argv:
    os.environ["CUDA_VISIBLE_DEVICES"] = ""

import json
import argparse
from pathlib import Path
import torch
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType,
)
from trl import SFTTrainer, SFTConfig


def load_jsonl(file_path: str) -> list[dict]:
    """JSONL íŒŒì¼ ë¡œë“œ"""
    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data


def format_messages_exaone(messages: list[dict]) -> str:
    """
    EXAONE 4.0 í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… (ë¹„ì¶”ë¡  ëª¨ë“œ)
    
    ë¹„ì¶”ë¡  ëª¨ë“œ: assistant ì‘ë‹µì— ë¹ˆ <think></think>ë¥¼ ë¶™ì—¬ì„œ
    ëª¨ë¸ì´ thinking ì—†ì´ ë°”ë¡œ ë‹µë³€í•˜ëŠ” íŒ¨í„´ì„ í•™ìŠµì‹œí‚´.
    
    í˜•ì‹:
      [|system|]{system_prompt}[|endofturn|]
      [|user|]{user_msg}[|endofturn|]
      [|assistant|]<think>
      </think>{assistant_msg}[|endofturn|]
    """
    formatted = ""
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        
        if role == "assistant":
            # ë¹„ì¶”ë¡  ëª¨ë“œ: ë¹ˆ thinking ë¸”ë¡ + ë°”ë¡œ ë‹µë³€
            formatted += f"[|{role}|]<think>\n</think>{content}[|endofturn|]\n"
        else:
            formatted += f"[|{role}|]{content}[|endofturn|]\n"
    return formatted


def main():
    parser = argparse.ArgumentParser(description="EXAONE 4.0 1.2B LoRA íŒŒì¸íŠœë‹")
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument("--model_name", type=str,
                        default="LGAI-EXAONE/EXAONE-4.0-1.2B",
                        help="ë² ì´ìŠ¤ ëª¨ë¸ (HuggingFace)")
    parser.add_argument("--output_dir", type=str,
                        default="./finetuning/output/exaone-counseling-lora",
                        help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    
    # ë°ì´í„° ì„¤ì •
    parser.add_argument("--train_data", type=str,
                        default="./finetuning/data/train_counseling.jsonl",
                        help="í•™ìŠµ ë°ì´í„° ê²½ë¡œ")
    parser.add_argument("--val_data", type=str,
                        default="./finetuning/data/val_counseling.jsonl",
                        help="ê²€ì¦ ë°ì´í„° ê²½ë¡œ")
    
    # LoRA ì„¤ì •
    # EXAONE 1.2BëŠ” Kanana 2.1Bë³´ë‹¤ ì‘ì•„ì„œ rankë¥¼ ì•½ê°„ ë†’ì—¬ë„ OK
    parser.add_argument("--lora_r", type=int, default=16, help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.05, help="LoRA dropout")
    
    # í•™ìŠµ ì„¤ì •
    parser.add_argument("--epochs", type=int, default=7,
                        help="ì—í¬í¬ ìˆ˜ (ì†ŒëŸ‰ ë°ì´í„°ëŠ” 5-10 ê¶Œì¥)")
    parser.add_argument("--batch_size", type=int, default=2, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning_rate", type=float, default=1e-4,
                        help="í•™ìŠµë¥  (ì†ŒëŸ‰ ë°ì´í„°ëŠ” 5e-5~2e-4)")
    parser.add_argument("--max_seq_length", type=int, default=1024,
                        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")
    parser.add_argument("--gradient_accumulation", type=int, default=4,
                        help="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  (effective batch = batch * accum)")
    
    # ì–‘ìí™”
    parser.add_argument("--use_4bit", action="store_true",
                        help="4ë¹„íŠ¸ ì–‘ìí™” QLoRA (RTX 2080 Ti 11GB ê¶Œì¥)")
    parser.add_argument("--use_8bit", action="store_true",
                        help="8ë¹„íŠ¸ ì–‘ìí™”")
    parser.add_argument("--cpu", action="store_true",
                        help="CPU í•™ìŠµ (í…ŒìŠ¤íŠ¸ìš©)")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ EXAONE 4.0 1.2B LoRA íŒŒì¸íŠœë‹ ì‹œì‘")
    print("=" * 60)
    print(f"  ëª¨ë¸: {args.model_name}")
    print(f"  LoRA rank: {args.lora_r}, alpha: {args.lora_alpha}")
    print(f"  ì—í¬í¬: {args.epochs}")
    print(f"  ë°°ì¹˜: {args.batch_size} Ã— {args.gradient_accumulation} = {args.batch_size * args.gradient_accumulation}")
    print(f"  í•™ìŠµë¥ : {args.learning_rate}")
    print(f"  ì–‘ìí™”: {'4bit' if args.use_4bit else '8bit' if args.use_8bit else 'FP32/BF16'}")
    print("=" * 60)
    
    # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("\nğŸ“š í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_name,
        trust_remote_code=True,
        padding_side="right"
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 2. ì–‘ìí™” ì„¤ì •
    bnb_config = None
    device_map = None
    
    if args.cpu:
        print("ğŸ–¥ï¸  CPU ëª¨ë“œ")
    elif args.use_4bit:
        print("ğŸ”§ 4ë¹„íŠ¸ QLoRA ì„¤ì • (RTX 2080 Ti 11GB ìµœì í™”)...")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )
        device_map = "auto"
    elif args.use_8bit:
        print("ğŸ”§ 8ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •...")
        bnb_config = BitsAndBytesConfig(load_in_8bit=True)
        device_map = "auto"
    else:
        # EXAONE 1.2B FP16ì€ ~2.5GB â†’ 11GB GPUì— ì—¬ìœ  ìˆìŒ
        device_map = "auto"
    
    # 3. ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘: {args.model_name}")
    print("   (EXAONE 4.0 1.2B: 30 layers, 2048 hidden, GQA 32/8)")
    
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        quantization_config=bnb_config,
        device_map=device_map,
        trust_remote_code=True,
        torch_dtype=torch.float32 if args.cpu else (torch.bfloat16 if not bnb_config else None),
    )
    
    if bnb_config:
        model = prepare_model_for_kbit_training(model)
    
    # 4. LoRA ì„¤ì •
    # EXAONE 4.0 ì•„í‚¤í…ì²˜ ëª¨ë“ˆëª… (transformers ëª¨ë¸ ì†ŒìŠ¤ í™•ì¸):
    #   Attention: q_proj, k_proj, v_proj, o_proj  (Exaone4Attention)
    #   MLP: gate_proj, up_proj, down_proj  (Exaone4MLP / Olmo2MLP ê¸°ë°˜ SwiGLU)
    print(f"\nğŸ”— LoRA ì–´ëŒ‘í„° ì„¤ì • (r={args.lora_r}, alpha={args.lora_alpha})...")
    lora_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",  # attention
            "gate_proj", "up_proj", "down_proj",       # MLP (SwiGLU)
        ],
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )
    
    model = get_peft_model(model, lora_config)
    
    # í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° ì¶œë ¥
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    pct = 100 * trainable_params / total_params
    print(f"ğŸ“Š í•™ìŠµ íŒŒë¼ë¯¸í„°: {trainable_params:,} / {total_params:,} ({pct:.2f}%)")
    
    # 5. ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"\nğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    train_data = load_jsonl(args.train_data)
    val_data = load_jsonl(args.val_data) if Path(args.val_data).exists() else None
    
    print(f"   í•™ìŠµ: {len(train_data)}ê°œ")
    if val_data:
        print(f"   ê²€ì¦: {len(val_data)}ê°œ")
    
    # EXAONE í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    def format_sample(sample):
        return {"text": format_messages_exaone(sample["messages"])}
    
    train_dataset = Dataset.from_list([format_sample(s) for s in train_data])
    val_dataset = Dataset.from_list([format_sample(s) for s in val_data]) if val_data else None
    
    # 6. í•™ìŠµ ì„¤ì •
    training_args = SFTConfig(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        learning_rate=args.learning_rate,
        warmup_ratio=0.15,
        lr_scheduler_type="cosine",
        weight_decay=0.01,
        logging_steps=5,
        save_strategy="epoch",
        eval_strategy="epoch" if val_dataset else "no",
        load_best_model_at_end=True if val_dataset else False,
        metric_for_best_model="eval_loss" if val_dataset else None,
        fp16=False,
        bf16=not args.cpu,
        max_length=args.max_seq_length,
        dataset_text_field="text",
        report_to="none",
        seed=42,
        use_cpu=args.cpu,
        gradient_checkpointing=not args.cpu,  # VRAM ì ˆì•½
    )
    
    # 7. íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        processing_class=tokenizer,
    )
    
    # 8. í•™ìŠµ ì‹œì‘
    print("\nğŸ¯ í•™ìŠµ ì‹œì‘...")
    trainer.train()
    
    # 9. ëª¨ë¸ ì €ì¥
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {args.output_dir}")
    trainer.save_model()
    tokenizer.save_pretrained(args.output_dir)
    
    print("\n" + "=" * 60)
    print("âœ… EXAONE 4.0 íŒŒì¸íŠœë‹ ì™„ë£Œ!")
    print("=" * 60)
    print(f"ğŸ“ ì¶œë ¥: {args.output_dir}")
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("  1. LoRA ë³‘í•© + GGUF ë³€í™˜:")
    print("     python merge_and_convert.py \\")
    print(f"       --base_model {args.model_name} \\")
    print(f"       --lora_path {args.output_dir} \\")
    print("       --model_name exaone-counseling")
    print()
    print("  2. Ollama ë“±ë¡:")
    print("     ollama create exaone-counseling -f models/Modelfile.exaone-1.2b")
    print()
    print("  3. í…ŒìŠ¤íŠ¸:")
    print("     ollama run exaone-counseling 'ì•ˆë…•í•˜ì„¸ìš”, ì–´ë¥´ì‹ '")


if __name__ == "__main__":
    main()

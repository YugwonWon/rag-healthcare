#!/bin/bash
# LoRA Î≥ëÌï© ‚Üí GGUF Î≥ÄÌôò ‚Üí Ollama Îì±Î°ù
export CUDA_VISIBLE_DEVICES=3
cd /data3/yugwon/projects/rag-healthcare

PYTHON=/data3/yugwon/projects/rag-healthcare/venv/bin/python
LORA_PATH=./finetuning/output/exaone-counseling-lora
BASE_MODEL=LGAI-EXAONE/EXAONE-4.0-1.2B
MERGED_DIR=./finetuning/output/exaone-counseling-merged
GGUF_OUT=./models/exaone-counseling-finetuned.gguf

echo "=== Step 1: LoRA Î≥ëÌï© ==="
echo "Start: $(date)"

$PYTHON -c "
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

lora_path = '$LORA_PATH'
base_model = '$BASE_MODEL'
output_dir = '$MERGED_DIR'

print('üìö ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú...')
tokenizer = AutoTokenizer.from_pretrained(lora_path, trust_remote_code=True)

print('ü§ñ Î≤†Ïù¥Ïä§ Î™®Îç∏ Î°úÎìú...')
base = AutoModelForCausalLM.from_pretrained(
    base_model,
    torch_dtype=torch.float16,
    device_map='auto',
    trust_remote_code=True
)

print('üîó LoRA Î≥ëÌï©...')
model = PeftModel.from_pretrained(base, lora_path)
model = model.merge_and_unload()

print(f'üíæ Ï†ÄÏû•: {output_dir}')
os.makedirs(output_dir, exist_ok=True)
model.save_pretrained(output_dir, safe_serialization=True)
tokenizer.save_pretrained(output_dir)
print('‚úÖ Step 1 ÏôÑÎ£å: Î≥ëÌï© ÏôÑÎ£å!')
"

if [ $? -ne 0 ]; then
    echo "‚ùå Î≥ëÌï© Ïã§Ìå®"
    exit 1
fi

echo ""
echo "=== Step 2: GGUF Î≥ÄÌôò ==="
echo "$(date)"

# llama.cpp Í≤ΩÎ°ú ÌÉêÏÉâ
LLAMA_CPP=""
for p in ~/llama.cpp /opt/llama.cpp /usr/local/llama.cpp; do
    if [ -f "$p/convert_hf_to_gguf.py" ]; then
        LLAMA_CPP=$p
        break
    fi
done

if [ -z "$LLAMA_CPP" ]; then
    echo "‚ö†Ô∏è llama.cpp ÏóÜÏùå - pip install llama-cpp-python ÏãúÎèÑ"
    
    # transformers + gguf Ìå®ÌÇ§ÏßÄÎ°ú Î≥ÄÌôò ÏãúÎèÑ
    $PYTHON -c "
import subprocess, sys
# gguf Ìå®ÌÇ§ÏßÄÍ∞Ä ÏûàÏúºÎ©¥ convert_hf_to_gguf.py ÎåÄÏã† ÏÇ¨Ïö©
try:
    import gguf
    print('gguf Ìå®ÌÇ§ÏßÄ Î∞úÍ≤¨:', gguf.__version__)
except ImportError:
    print('gguf Ìå®ÌÇ§ÏßÄ ÏÑ§Ïπò Ï§ë...')
    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'gguf', '-q'])
"
    
    # huggingfaceÏóêÏÑú ÏßÅÏ†ë GGUF export ÏãúÎèÑ (transformers >= 4.44)
    $PYTHON -c "
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

print('üîÑ GGUF Î≥ÄÌôò ÏãúÎèÑ (transformers export)...')
model_path = '$MERGED_DIR'
output_path = '$GGUF_OUT'

try:
    from transformers.gguf import convert_to_gguf
    convert_to_gguf(model_path, output_path, quantization='q4_k_m')
    print('‚úÖ GGUF Î≥ÄÌôò ÏôÑÎ£å:', output_path)
except (ImportError, AttributeError) as e:
    print(f'‚ö†Ô∏è transformers GGUF export Î∂àÍ∞Ä: {e}')
    print('üí° ÎåÄÏïà: llama-cpp-pythonÏúºÎ°ú Î≥ÄÌôòÌï©ÎãàÎã§')
    
    # ÏµúÌõÑ ÏàòÎã®: llama-cpp-python convert
    import subprocess, sys
    try:
        result = subprocess.run([
            sys.executable, '-m', 'llama_cpp.convert',
            model_path, '--outfile', output_path, '--outtype', 'q4_k_m'
        ], capture_output=True, text=True)
        if result.returncode == 0:
            print('‚úÖ GGUF Î≥ÄÌôò ÏôÑÎ£å:', output_path)
        else:
            print('‚ùå Î≥ÄÌôò Ïã§Ìå®:', result.stderr[:500])
            print('')
            print('ÏàòÎèô Î≥ÄÌôòÏù¥ ÌïÑÏöîÌï©ÎãàÎã§:')
            print('  pip install llama-cpp-python')
            print(f'  python -m llama_cpp.convert {model_path} --outfile {output_path} --outtype q4_k_m')
    except Exception as e2:
        print(f'‚ùå llama-cpp-pythonÎèÑ ÏóÜÏùå: {e2}')
        print('Î≥ëÌï©Îêú Î™®Îç∏ÏùÄ Ïó¨Í∏∞ ÏûàÏäµÎãàÎã§:', model_path)
        sys.exit(1)
"
else
    echo "llama.cpp Î∞úÍ≤¨: $LLAMA_CPP"
    FP16_GGUF=${GGUF_OUT%.gguf}-fp16.gguf
    
    $PYTHON $LLAMA_CPP/convert_hf_to_gguf.py $MERGED_DIR --outfile $FP16_GGUF --outtype f16
    
    if [ -f "$LLAMA_CPP/build/bin/llama-quantize" ]; then
        $LLAMA_CPP/build/bin/llama-quantize $FP16_GGUF $GGUF_OUT Q4_K_M
        rm -f $FP16_GGUF
        echo "‚úÖ GGUF ÏñëÏûêÌôî ÏôÑÎ£å: $GGUF_OUT"
    else
        mv $FP16_GGUF $GGUF_OUT
        echo "‚úÖ GGUF (FP16) ÏôÑÎ£å: $GGUF_OUT"
    fi
fi

echo ""
echo "=== Step 3: Ollama Îì±Î°ù ==="
echo "$(date)"

# GGUF ÌååÏùºÏù¥ ÏûàÏúºÎ©¥ Îì±Î°ù, ÏóÜÏúºÎ©¥ Í∏∞Ï°¥ base GGUF ÏÇ¨Ïö©
if [ -f "$GGUF_OUT" ]; then
    GGUF_PATH=$(realpath $GGUF_OUT)
else
    echo "‚ö†Ô∏è ÌååÏù∏ÌäúÎãù GGUF ÏóÜÏùå, Í∏∞Ï°¥ base GGUF ÏÇ¨Ïö©"
    GGUF_PATH=$(realpath ./models/EXAONE-4.0-1.2B-Q4_K_M.gguf)
fi

echo "GGUF Í≤ΩÎ°ú: $GGUF_PATH"

# Modelfile ÏÉùÏÑ±
cat > ./models/Modelfile.exaone-counseling << 'MODELFILE_END'
FROM GGUF_PLACEHOLDER

SYSTEM "ÎãπÏã†ÏùÄ ÎÖ∏Ïù∏Í±¥Í∞ïÏ†ÑÎ¨∏ÏÉÅÎã¥ÏÇ¨ÏûÖÎãàÎã§. Î∞òÎìúÏãú ÌïúÍµ≠Ïñ¥Î°úÎßå ÏùëÎãµÌïòÏÑ∏Ïöî. ÌïúÏûêÎ•º ÏÇ¨Ïö©ÌïòÏßÄ ÎßàÏÑ∏Ïöî. 2~3Î¨∏Ïû•ÏúºÎ°ú Í∞ÑÍ≤∞ÌïòÍ≤å ÎãµÎ≥ÄÌïòÍ≥†, Í≥µÍ∞ê ÌõÑ ÏßàÎ¨∏ÏúºÎ°ú Î¨∏Ï†úÎ•º ÌååÏïÖÌïòÏÑ∏Ïöî. ÏùºÏÉÅÏóêÏÑú Ïã§Ï≤úÌï† Ïàò ÏûàÎäî Í±¥Í∞ï ÏäµÍ¥ÄÏùÑ ÏïàÎÇ¥ÌïòÍ≥†, Ïã¨Í∞ÅÌïú Í≤ΩÏö∞ÏóêÎßå Î≥ëÏõê ÏßÑÎ£åÎ•º Í∂åÏú†ÌïòÏÑ∏Ïöî."

PARAMETER temperature 0.1
PARAMETER top_k 40
PARAMETER top_p 0.9
PARAMETER num_predict 256
PARAMETER repeat_penalty 1.1
PARAMETER stop "[|endofturn|]"

TEMPLATE """{{- if .System }}[|system|]{{ .System }}[|endofturn|]
{{- end }}{{- range .Messages }}
{{- if eq .Role "user" }}[|user|]{{ .Content }}[|endofturn|]
{{- else if eq .Role "assistant" }}[|assistant|]{{ .Content }}[|endofturn|]
{{- end }}{{- end }}[|assistant|]<think>
</think>"""
MODELFILE_END

# GGUF Í≤ΩÎ°ú ÏπòÌôò
sed -i "s|GGUF_PLACEHOLDER|$GGUF_PATH|g" ./models/Modelfile.exaone-counseling

echo "Modelfile ÎÇ¥Ïö©:"
cat ./models/Modelfile.exaone-counseling

echo ""
echo "Ollama Îì±Î°ù Ï§ë..."
ollama create exaone-counseling -f ./models/Modelfile.exaone-counseling

echo ""
echo "=== ÏôÑÎ£å ==="
echo "End: $(date)"

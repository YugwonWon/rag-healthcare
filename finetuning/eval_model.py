"""
ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
"""

import argparse
import json
from typing import Optional
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from tqdm import tqdm


def load_model(
    model_path: str,
    adapter_path: Optional[str] = None,
    use_4bit: bool = False,
):
    """ëª¨ë¸ ë¡œë“œ"""
    print(f"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
    
    quantization_config = None
    if use_4bit:
        from transformers import BitsAndBytesConfig
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )
    
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=quantization_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16 if not quantization_config else None,
    )
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True,
    )
    
    if adapter_path:
        print(f"ğŸ”„ ì–´ëŒ‘í„° ë¡œë“œ ì¤‘: {adapter_path}")
        model = PeftModel.from_pretrained(model, adapter_path)
    
    return model, tokenizer


def generate_response(
    model,
    tokenizer,
    prompt: str,
    max_new_tokens: int = 512,
    temperature: float = 0.7,
):
    """ì‘ë‹µ ìƒì„±"""
    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹˜ë§¤ë…¸ì¸ì„ ëŒë³´ëŠ” ë”°ëœ»í•˜ê³  ì¹œì ˆí•œ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
        {"role": "user", "content": prompt}
    ]
    
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    return response


def evaluate_on_test_set(
    model,
    tokenizer,
    test_data_path: str,
    output_path: str,
):
    """í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ í‰ê°€"""
    print(f"ğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ: {test_data_path}")
    
    with open(test_data_path, "r", encoding="utf-8") as f:
        test_data = [json.loads(line) for line in f if line.strip()]
    
    results = []
    
    for item in tqdm(test_data, desc="í‰ê°€ ì¤‘"):
        messages = item.get("messages", [])
        
        # ë§ˆì§€ë§‰ assistant ì‘ë‹µ ì „ê¹Œì§€ì˜ ëŒ€í™” ì¶”ì¶œ
        prompt_messages = []
        expected_response = None
        
        for msg in messages:
            if msg["role"] == "assistant" and expected_response is None:
                expected_response = msg["content"]
            else:
                prompt_messages.append(msg)
        
        if not expected_response:
            continue
        
        # user ë©”ì‹œì§€ ì¶”ì¶œ
        user_message = next((m["content"] for m in reversed(prompt_messages) if m["role"] == "user"), None)
        if not user_message:
            continue
        
        # ì‘ë‹µ ìƒì„±
        generated_response = generate_response(model, tokenizer, user_message)
        
        results.append({
            "input": user_message,
            "expected": expected_response,
            "generated": generated_response,
        })
    
    # ê²°ê³¼ ì €ì¥
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… í‰ê°€ ê²°ê³¼ ì €ì¥: {output_path}")
    print(f"   ì´ {len(results)}ê°œ ìƒ˜í”Œ í‰ê°€ë¨")
    
    return results


def interactive_eval(model, tokenizer):
    """ëŒ€í™”í˜• í‰ê°€"""
    print("\nğŸ—£ï¸ ëŒ€í™”í˜• í‰ê°€ ëª¨ë“œ (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")
    print("-" * 50)
    
    while True:
        user_input = input("\nğŸ‘¤ ì‚¬ìš©ì: ").strip()
        
        if user_input.lower() in ["quit", "exit", "q"]:
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if not user_input:
            continue
        
        response = generate_response(model, tokenizer, user_input)
        print(f"\nğŸ¤– AI: {response}")


def calculate_metrics(results: list[dict]) -> dict:
    """ê°„ë‹¨í•œ ë©”íŠ¸ë¦­ ê³„ì‚°"""
    from collections import Counter
    
    # ì‘ë‹µ ê¸¸ì´ í†µê³„
    gen_lengths = [len(r["generated"]) for r in results]
    exp_lengths = [len(r["expected"]) for r in results]
    
    # í‚¤ì›Œë“œ ì¼ì¹˜ìœ¨ (ê°„ë‹¨í•œ í‰ê°€)
    keyword_matches = 0
    important_keywords = ["ì–´ë¥´ì‹ ", "ë„¤", "ë“œì„¸ìš”", "ì¢‹ì•„ìš”", "ê´œì°®ì•„ìš”"]
    
    for r in results:
        gen = r["generated"].lower()
        for keyword in important_keywords:
            if keyword in gen:
                keyword_matches += 1
                break
    
    return {
        "num_samples": len(results),
        "avg_generated_length": sum(gen_lengths) / len(gen_lengths) if gen_lengths else 0,
        "avg_expected_length": sum(exp_lengths) / len(exp_lengths) if exp_lengths else 0,
        "keyword_match_rate": keyword_matches / len(results) if results else 0,
    }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ëª¨ë¸ í‰ê°€")
    
    parser.add_argument("--model_path", type=str, required=True, help="ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--adapter_path", type=str, help="LoRA ì–´ëŒ‘í„° ê²½ë¡œ")
    parser.add_argument("--test_data", type=str, help="í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ")
    parser.add_argument("--output", type=str, default="eval_results.json", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ")
    parser.add_argument("--interactive", action="store_true", help="ëŒ€í™”í˜• í‰ê°€ ëª¨ë“œ")
    parser.add_argument("--use_4bit", action="store_true", help="4ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©")
    
    args = parser.parse_args()
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model(
        args.model_path,
        args.adapter_path,
        args.use_4bit,
    )
    
    if args.interactive:
        interactive_eval(model, tokenizer)
    elif args.test_data:
        results = evaluate_on_test_set(
            model, tokenizer,
            args.test_data,
            args.output,
        )
        metrics = calculate_metrics(results)
        print("\nğŸ“Š í‰ê°€ ë©”íŠ¸ë¦­:")
        for key, value in metrics.items():
            print(f"   {key}: {value:.4f}" if isinstance(value, float) else f"   {key}: {value}")
    else:
        print("--test_data ë˜ëŠ” --interactive ì˜µì…˜ì„ ì§€ì •í•˜ì„¸ìš”.")

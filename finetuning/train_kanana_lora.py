"""
Kanana ëª¨ë¸ LoRA íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
ê²½ëŸ‰ íŒŒì¸íŠœë‹ìœ¼ë¡œ ëŒ€í™” ìŠ¤íƒ€ì¼ë§Œ í•™ìŠµ (ê³¼ì í•© ë°©ì§€ ìµœì í™”)

157ê°œ ì†ŒëŸ‰ ë°ì´í„° ê¸°ì¤€ ìµœì í™”:
- LoRA r=4 (ìµœì†Œ rankë¡œ ìŠ¤íƒ€ì¼ë§Œ í•™ìŠµ)
- ë†’ì€ dropout(0.15) + label smoothing(0.1)
- ë‚®ì€ LR(2e-5) + cosine decay + early stopping
- epochë‹¹ ~20 steps â†’ 3 epochs = ~60 steps

ì‚¬ìš©ë²•:
    python train_kanana_lora.py --use_4bit --epochs 3
    python train_kanana_lora.py --cpu --epochs 3  # CPU ëª¨ë“œ
"""

import os
import sys

# CPU ëª¨ë“œ ì²´í¬ (--cpu ì¸ìê°€ ìˆìœ¼ë©´ CUDA ë¹„í™œì„±í™”)
if "--cpu" in sys.argv:
    os.environ["CUDA_VISIBLE_DEVICES"] = ""

import json
import argparse
from pathlib import Path
import torch
from datasets import load_dataset, Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType,
)
from trl import SFTTrainer, SFTConfig
from transformers import EarlyStoppingCallback


def load_jsonl(file_path: str) -> list[dict]:
    """JSONL íŒŒì¼ ë¡œë“œ"""
    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data


def format_messages_kanana(messages: list[dict]) -> str:
    """
    Kanana ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
    KananaëŠ” ChatML í˜•ì‹ ì‚¬ìš©
    """
    formatted = ""
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        
        if role == "system":
            formatted += f"<|im_start|>system\n{content}<|im_end|>\n"
        elif role == "user":
            formatted += f"<|im_start|>user\n{content}<|im_end|>\n"
        elif role == "assistant":
            formatted += f"<|im_start|>assistant\n{content}<|im_end|>\n"
    
    return formatted


def main():
    parser = argparse.ArgumentParser(description="Kanana LoRA íŒŒì¸íŠœë‹")
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument("--model_name", type=str, 
                        default="kakaocorp/kanana-nano-2.1b-instruct",
                        help="ë² ì´ìŠ¤ ëª¨ë¸")
    parser.add_argument("--output_dir", type=str,
                        default="./finetuning/output/kanana-counseling-lora",
                        help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    
    # ë°ì´í„° ì„¤ì •
    parser.add_argument("--train_data", type=str,
                        default="./finetuning/data/train_counseling.jsonl",
                        help="í•™ìŠµ ë°ì´í„° ê²½ë¡œ")
    parser.add_argument("--val_data", type=str,
                        default="./finetuning/data/val_counseling.jsonl",
                        help="ê²€ì¦ ë°ì´í„° ê²½ë¡œ")
    
    # LoRA ì„¤ì • (ê²½ëŸ‰ - ìŠ¤íƒ€ì¼ë§Œ í•™ìŠµ, ê³¼ì í•© ë°©ì§€)
    parser.add_argument("--lora_r", type=int, default=4, help="LoRA rank (ìµœì†Œê°’ìœ¼ë¡œ ìŠ¤íƒ€ì¼ë§Œ í•™ìŠµ)")
    parser.add_argument("--lora_alpha", type=int, default=8, help="LoRA alpha (rì˜ 2ë°°)")
    parser.add_argument("--lora_dropout", type=float, default=0.15, help="LoRA dropout (ì†ŒëŸ‰ ë°ì´í„°â†’ë†’ì€ dropout)")
    
    # í•™ìŠµ ì„¤ì • (157ìƒ˜í”Œ ê³¼ì í•© ë°©ì§€ ìµœì í™”)
    parser.add_argument("--epochs", type=int, default=3, help="ì—í¬í¬ ìˆ˜ (early stoppingê³¼ í•¨ê»˜ ì‚¬ìš©)")
    parser.add_argument("--batch_size", type=int, default=2, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="í•™ìŠµë¥  (ì†ŒëŸ‰ ë°ì´í„°â†’ë‚®ì€ LRë¡œ ê³¼ì í•© ë°©ì§€)")
    parser.add_argument("--max_seq_length", type=int, default=512, help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (í‰ê·  443ì, 512ë¡œ ì¶©ë¶„)")
    parser.add_argument("--gradient_accumulation", type=int, default=4, help="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  (effective batch=8)")
    parser.add_argument("--label_smoothing", type=float, default=0.1, help="ë¼ë²¨ ìŠ¤ë¬´ë”© (ê³¼ì í•© ë°©ì§€)")
    parser.add_argument("--early_stopping_patience", type=int, default=2, help="Early stopping patience")
    
    # ì–‘ìí™”
    parser.add_argument("--use_4bit", action="store_true", help="4ë¹„íŠ¸ ì–‘ìí™” (QLoRA)")
    parser.add_argument("--use_8bit", action="store_true", help="8ë¹„íŠ¸ ì–‘ìí™”")
    parser.add_argument("--cpu", action="store_true", help="CPUë¡œ í•™ìŠµ (ì–‘ìí™” ì—†ìŒ)")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ Kanana LoRA íŒŒì¸íŠœë‹ ì‹œì‘ (ê³¼ì í•© ë°©ì§€ ìµœì í™”)")
    print("=" * 60)
    print(f"  ëª¨ë¸: {args.model_name}")
    print(f"  LoRA: r={args.lora_r}, alpha={args.lora_alpha}, dropout={args.lora_dropout}")
    print(f"  ì—í¬í¬: {args.epochs} (early stopping patience={args.early_stopping_patience})")
    print(f"  ë°°ì¹˜: {args.batch_size} Ã— {args.gradient_accumulation} = {args.batch_size * args.gradient_accumulation}")
    print(f"  í•™ìŠµë¥ : {args.learning_rate}, label smoothing: {args.label_smoothing}")
    print(f"  ì–‘ìí™”: {'4bit' if args.use_4bit else '8bit' if args.use_8bit else 'FP32/BF16'}")
    print("=" * 60)
    
    # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("\nğŸ“š í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_name,
        trust_remote_code=True,
        padding_side="right"
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 2. ì–‘ìí™” ì„¤ì •
    bnb_config = None
    device_map = None  # CPUì—ì„œëŠ” device_map ì‚¬ìš© ì•ˆí•¨
    
    if args.cpu:
        print("ğŸ–¥ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰...")
    elif args.use_4bit:
        print("ğŸ”§ 4ë¹„íŠ¸ ì–‘ìí™” (QLoRA) ì„¤ì •...")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )
        device_map = "auto"
    elif args.use_8bit:
        print("ğŸ”§ 8ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •...")
        bnb_config = BitsAndBytesConfig(load_in_8bit=True)
        device_map = "auto"
    
    # 3. ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘: {args.model_name}")
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        quantization_config=bnb_config,
        device_map=device_map,
        trust_remote_code=True,
        torch_dtype=torch.float32 if args.cpu else (torch.bfloat16 if not bnb_config else None),
    )
    
    if bnb_config:
        model = prepare_model_for_kbit_training(model)
    
    # 4. LoRA ì„¤ì •
    print(f"\nğŸ”— LoRA ì–´ëŒ‘í„° ì„¤ì • (r={args.lora_r}, alpha={args.lora_alpha})...")
    lora_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )
    
    model = get_peft_model(model, lora_config)
    
    # í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° ì¶œë ¥
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)")
    
    # 5. ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"\nğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    train_data = load_jsonl(args.train_data)
    val_data = load_jsonl(args.val_data) if Path(args.val_data).exists() else None
    
    print(f"   í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ")
    if val_data:
        print(f"   ê²€ì¦ ë°ì´í„°: {len(val_data)}ê°œ")
    
    # ë°ì´í„°ì…‹ ë³€í™˜
    def format_sample(sample):
        return {"text": format_messages_kanana(sample["messages"])}
    
    train_dataset = Dataset.from_list([format_sample(s) for s in train_data])
    val_dataset = Dataset.from_list([format_sample(s) for s in val_data]) if val_data else None
    
    # 6. í•™ìŠµ ì„¤ì • (ì†ŒëŸ‰ ë°ì´í„° ê³¼ì í•© ë°©ì§€ ìµœì í™”)
    training_args = SFTConfig(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        learning_rate=args.learning_rate,
        warmup_ratio=0.2,           # ë†’ì€ warmup (ì†ŒëŸ‰ ë°ì´í„°â†’ì²œì²œíˆ ì‹œì‘)
        weight_decay=0.1,            # ê°•í•œ L2 ì •ê·œí™”
        lr_scheduler_type="cosine",  # ìì—°ìŠ¤ëŸ¬ìš´ LR ê°ì†Œ
        label_smoothing_factor=args.label_smoothing,  # ë¼ë²¨ ìŠ¤ë¬´ë”© (ê³¼ì í•© ë°©ì§€)
        logging_steps=5,             # epochë‹¹ ~20 steps â†’ ìì£¼ ë¡œê¹…
        save_strategy="epoch",
        eval_strategy="epoch" if val_dataset else "no",
        load_best_model_at_end=True if val_dataset else False,  # ìµœì  ì²´í¬í¬ì¸íŠ¸ ìë™ ì„ íƒ
        metric_for_best_model="eval_loss" if val_dataset else None,
        greater_is_better=False if val_dataset else None,
        save_total_limit=3,
        fp16=False,
        bf16=not args.cpu,
        max_length=args.max_seq_length,
        dataset_text_field="text",
        report_to="none",
        seed=42,
        use_cpu=args.cpu,
        gradient_checkpointing=not args.cpu,  # VRAM ì ˆì•½
        max_grad_norm=1.0,           # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    )
    
    # 7. íŠ¸ë ˆì´ë„ˆ ìƒì„± (early stopping ì½œë°± í¬í•¨)
    callbacks = []
    if val_dataset:
        callbacks.append(
            EarlyStoppingCallback(early_stopping_patience=args.early_stopping_patience)
        )
        print(f"ğŸ“Œ Early stopping í™œì„±í™” (patience={args.early_stopping_patience})")
    
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        processing_class=tokenizer,
        callbacks=callbacks,
    )
    
    # 8. í•™ìŠµ ì‹œì‘
    print("\nğŸ¯ í•™ìŠµ ì‹œì‘...")
    trainer.train()
    
    # 9. ëª¨ë¸ ì €ì¥
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {args.output_dir}")
    trainer.save_model()
    tokenizer.save_pretrained(args.output_dir)
    
    # í•™ìŠµ ê²°ê³¼ ì¶œë ¥
    train_result = trainer.state.log_history
    print("\n" + "=" * 60)
    print("âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!")
    print("=" * 60)
    print(f"ğŸ“ ì¶œë ¥ ê²½ë¡œ: {args.output_dir}")
    
    # ìµœì¢… ì†ì‹¤ ì¶œë ¥
    train_losses = [h['loss'] for h in train_result if 'loss' in h]
    eval_losses = [h['eval_loss'] for h in train_result if 'eval_loss' in h]
    if train_losses:
        print(f"ğŸ“‰ ìµœì¢… train loss: {train_losses[-1]:.4f}")
    if eval_losses:
        print(f"ğŸ“‰ ìµœì¢… eval loss: {eval_losses[-1]:.4f}")
        print(f"ğŸ“‰ ìµœì  eval loss: {min(eval_losses):.4f}")
    
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("  1. LoRA ë³‘í•© + GGUF ë³€í™˜:")
    print("     python merge_and_convert.py \\")
    print(f"       --base_model {args.model_name} \\")
    print(f"       --lora_path {args.output_dir} \\")
    print("       --model_name kanana-counseling")
    print()
    print("  2. Ollama ë“±ë¡:")
    print("     ollama create kanana-counseling -f models/Modelfile.kanana-counseling")


if __name__ == "__main__":
    main()

"""
Kanana ëª¨ë¸ LoRA íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
ê²½ëŸ‰ íŒŒì¸íŠœë‹ìœ¼ë¡œ ëŒ€í™” ìŠ¤íƒ€ì¼ë§Œ í•™ìŠµ

ì‚¬ìš©ë²•:
    python train_kanana_lora.py --epochs 3 --batch_size 2
    python train_kanana_lora.py --cpu --epochs 3  # CPU ëª¨ë“œ
"""

import os
import sys

# CPU ëª¨ë“œ ì²´í¬ (--cpu ì¸ìê°€ ìˆìœ¼ë©´ CUDA ë¹„í™œì„±í™”)
if "--cpu" in sys.argv:
    os.environ["CUDA_VISIBLE_DEVICES"] = ""

import json
import argparse
from pathlib import Path
import torch
from datasets import load_dataset, Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType,
)
from trl import SFTTrainer, SFTConfig


def load_jsonl(file_path: str) -> list[dict]:
    """JSONL íŒŒì¼ ë¡œë“œ"""
    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data


def format_messages_kanana(messages: list[dict]) -> str:
    """
    Kanana ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
    KananaëŠ” ChatML í˜•ì‹ ì‚¬ìš©
    """
    formatted = ""
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        
        if role == "system":
            formatted += f"<|im_start|>system\n{content}<|im_end|>\n"
        elif role == "user":
            formatted += f"<|im_start|>user\n{content}<|im_end|>\n"
        elif role == "assistant":
            formatted += f"<|im_start|>assistant\n{content}<|im_end|>\n"
    
    return formatted


def main():
    parser = argparse.ArgumentParser(description="Kanana LoRA íŒŒì¸íŠœë‹")
    
    # ëª¨ë¸ ì„¤ì •
    parser.add_argument("--model_name", type=str, 
                        default="kakaocorp/kanana-nano-2.1b-instruct",
                        help="ë² ì´ìŠ¤ ëª¨ë¸")
    parser.add_argument("--output_dir", type=str,
                        default="./finetuning/output/kanana-counseling-lora",
                        help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    
    # ë°ì´í„° ì„¤ì •
    parser.add_argument("--train_data", type=str,
                        default="./finetuning/data/train_counseling.jsonl",
                        help="í•™ìŠµ ë°ì´í„° ê²½ë¡œ")
    parser.add_argument("--val_data", type=str,
                        default="./finetuning/data/val_counseling.jsonl",
                        help="ê²€ì¦ ë°ì´í„° ê²½ë¡œ")
    
    # LoRA ì„¤ì • (ê²½ëŸ‰ - ìŠ¤íƒ€ì¼ë§Œ í•™ìŠµ)
    parser.add_argument("--lora_r", type=int, default=8, help="LoRA rank (ì‘ì„ìˆ˜ë¡ ê²½ëŸ‰)")
    parser.add_argument("--lora_alpha", type=int, default=16, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.05, help="LoRA dropout")
    
    # í•™ìŠµ ì„¤ì •
    parser.add_argument("--epochs", type=int, default=7, help="ì—í¬í¬ ìˆ˜ (ì†ŒëŸ‰ ë°ì´í„°ëŠ” 5-10 ê¶Œì¥)")
    parser.add_argument("--batch_size", type=int, default=2, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="í•™ìŠµë¥  (ì†ŒëŸ‰ ë°ì´í„°ëŠ” 5e-5~2e-4 ê¶Œì¥)")
    parser.add_argument("--max_seq_length", type=int, default=1024, help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")
    parser.add_argument("--gradient_accumulation", type=int, default=4, help="ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ")
    
    # ì–‘ìí™”
    parser.add_argument("--use_4bit", action="store_true", help="4ë¹„íŠ¸ ì–‘ìí™” (QLoRA)")
    parser.add_argument("--use_8bit", action="store_true", help="8ë¹„íŠ¸ ì–‘ìí™”")
    parser.add_argument("--cpu", action="store_true", help="CPUë¡œ í•™ìŠµ (ì–‘ìí™” ì—†ìŒ)")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ Kanana LoRA íŒŒì¸íŠœë‹ ì‹œì‘")
    print("=" * 60)
    print(f"  ëª¨ë¸: {args.model_name}")
    print(f"  LoRA rank: {args.lora_r}")
    print(f"  ì—í¬í¬: {args.epochs}")
    print(f"  ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    print("=" * 60)
    
    # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("\nğŸ“š í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_name,
        trust_remote_code=True,
        padding_side="right"
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 2. ì–‘ìí™” ì„¤ì •
    bnb_config = None
    device_map = None  # CPUì—ì„œëŠ” device_map ì‚¬ìš© ì•ˆí•¨
    
    if args.cpu:
        print("ğŸ–¥ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰...")
    elif args.use_4bit:
        print("ğŸ”§ 4ë¹„íŠ¸ ì–‘ìí™” (QLoRA) ì„¤ì •...")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )
        device_map = "auto"
    elif args.use_8bit:
        print("ğŸ”§ 8ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •...")
        bnb_config = BitsAndBytesConfig(load_in_8bit=True)
        device_map = "auto"
    
    # 3. ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘: {args.model_name}")
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        quantization_config=bnb_config,
        device_map=device_map,
        trust_remote_code=True,
        torch_dtype=torch.float32 if args.cpu else (torch.bfloat16 if not bnb_config else None),
    )
    
    if bnb_config:
        model = prepare_model_for_kbit_training(model)
    
    # 4. LoRA ì„¤ì •
    print(f"\nğŸ”— LoRA ì–´ëŒ‘í„° ì„¤ì • (r={args.lora_r}, alpha={args.lora_alpha})...")
    lora_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )
    
    model = get_peft_model(model, lora_config)
    
    # í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„° ì¶œë ¥
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)")
    
    # 5. ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"\nğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    train_data = load_jsonl(args.train_data)
    val_data = load_jsonl(args.val_data) if Path(args.val_data).exists() else None
    
    print(f"   í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ")
    if val_data:
        print(f"   ê²€ì¦ ë°ì´í„°: {len(val_data)}ê°œ")
    
    # ë°ì´í„°ì…‹ ë³€í™˜
    def format_sample(sample):
        return {"text": format_messages_kanana(sample["messages"])}
    
    train_dataset = Dataset.from_list([format_sample(s) for s in train_data])
    val_dataset = Dataset.from_list([format_sample(s) for s in val_data]) if val_data else None
    
    # 6. í•™ìŠµ ì„¤ì •
    training_args = SFTConfig(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation,
        learning_rate=args.learning_rate,
        warmup_ratio=0.15,
        lr_scheduler_type="cosine",
        logging_steps=10,
        save_strategy="epoch",
        eval_strategy="epoch" if val_dataset else "no",
        fp16=False,
        bf16=not args.cpu,  # CPUì—ì„œëŠ” bf16 ë¹„í™œì„±í™”
        max_length=args.max_seq_length,
        dataset_text_field="text",
        report_to="none",
        seed=42,
        use_cpu=args.cpu,  # CPU ëª¨ë“œ
    )
    
    # 7. íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        processing_class=tokenizer,
    )
    
    # 8. í•™ìŠµ ì‹œì‘
    print("\nğŸ¯ í•™ìŠµ ì‹œì‘...")
    trainer.train()
    
    # 9. ëª¨ë¸ ì €ì¥
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘: {args.output_dir}")
    trainer.save_model()
    tokenizer.save_pretrained(args.output_dir)
    
    print("\n" + "=" * 60)
    print("âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!")
    print("=" * 60)
    print(f"ğŸ“ ì¶œë ¥ ê²½ë¡œ: {args.output_dir}")
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("  1. GGUF ë³€í™˜: python merge_and_convert.py")
    print("  2. Ollama ë“±ë¡: ollama create kanana-counseling -f Modelfile")


if __name__ == "__main__":
    main()

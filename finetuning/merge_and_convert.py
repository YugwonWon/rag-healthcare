"""
LoRA ì–´ëŒ‘í„° ë³‘í•© ë° GGUF ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
íŒŒì¸íŠœë‹ëœ Kanana ëª¨ë¸ì„ Ollamaì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜

ì‚¬ìš©ë²•:
    python merge_and_convert.py --lora_path ./finetuning/output/kanana-counseling-lora
"""

import os
import argparse
import subprocess
from pathlib import Path
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel


def merge_lora_adapter(base_model: str, lora_path: str, output_path: str):
    """LoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©"""
    print(f"\nğŸ”— LoRA ì–´ëŒ‘í„° ë³‘í•© ì¤‘...")
    print(f"   ë² ì´ìŠ¤ ëª¨ë¸: {base_model}")
    print(f"   LoRA ê²½ë¡œ: {lora_path}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(lora_path, trust_remote_code=True)
    
    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    base = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # LoRA ì–´ëŒ‘í„° ë¡œë“œ ë° ë³‘í•©
    model = PeftModel.from_pretrained(base, lora_path)
    model = model.merge_and_unload()
    
    # ì €ì¥
    print(f"ğŸ’¾ ë³‘í•©ëœ ëª¨ë¸ ì €ì¥: {output_path}")
    model.save_pretrained(output_path, safe_serialization=True)
    tokenizer.save_pretrained(output_path)
    
    print("âœ… ë³‘í•© ì™„ë£Œ!")
    return output_path


def convert_to_gguf(model_path: str, output_path: str, quantization: str = "q4_k_m"):
    """
    HuggingFace ëª¨ë¸ì„ GGUF í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    llama.cpp í•„ìš”
    """
    print(f"\nğŸ”„ GGUF ë³€í™˜ ì¤‘ (ì–‘ìí™”: {quantization})...")
    
    # llama.cpp ê²½ë¡œ í™•ì¸
    llama_cpp_path = os.environ.get("LLAMA_CPP_PATH", "~/llama.cpp")
    llama_cpp_path = os.path.expanduser(llama_cpp_path)
    
    convert_script = Path(llama_cpp_path) / "convert_hf_to_gguf.py"
    quantize_bin = Path(llama_cpp_path) / "build" / "bin" / "llama-quantize"
    
    if not convert_script.exists():
        print(f"âš ï¸ llama.cppë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {llama_cpp_path}")
        print("   ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("   git clone https://github.com/ggerganov/llama.cpp")
        print("   cd llama.cpp && make -j")
        return None
    
    # FP16 GGUF ë³€í™˜
    fp16_path = output_path.replace(".gguf", "-fp16.gguf")
    
    cmd_convert = [
        "python", str(convert_script),
        model_path,
        "--outfile", fp16_path,
        "--outtype", "f16"
    ]
    
    print(f"   ì‹¤í–‰: {' '.join(cmd_convert)}")
    result = subprocess.run(cmd_convert, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {result.stderr}")
        return None
    
    # ì–‘ìí™”
    if quantization != "f16":
        cmd_quantize = [
            str(quantize_bin),
            fp16_path,
            output_path,
            quantization.upper()
        ]
        
        print(f"   ì–‘ìí™”: {' '.join(cmd_quantize)}")
        result = subprocess.run(cmd_quantize, capture_output=True, text=True)
        
        if result.returncode != 0:
            print(f"âŒ ì–‘ìí™” ì‹¤íŒ¨: {result.stderr}")
            return fp16_path
        
        # FP16 íŒŒì¼ ì‚­ì œ
        os.remove(fp16_path)
    else:
        output_path = fp16_path
    
    print(f"âœ… GGUF ë³€í™˜ ì™„ë£Œ: {output_path}")
    return output_path


def create_ollama_modelfile(gguf_path: str, output_path: str, model_name: str):
    """Ollama Modelfile ìƒì„±"""
    modelfile_content = f'''# Kanana ìƒë‹´ ëª¨ë¸ - íŒŒì¸íŠœë‹ë¨
FROM {gguf_path}

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM """ë‹¹ì‹ ì€ ë…¸ì¸ê±´ê°•ì „ë¬¸ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
- 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€
- ê³µê° í›„ ì§ˆë¬¸ìœ¼ë¡œ ë¬¸ì œë¥¼ íŒŒì•…
- ì¼ìƒì—ì„œ ì‹¤ì²œí•  ìˆ˜ ìˆëŠ” ê±´ê°• ìŠµê´€ ì•ˆë‚´
- ì‹¬ê°í•œ ê²½ìš°ì—ë§Œ ë³‘ì› ì§„ë£Œ ê¶Œìœ """

# íŒŒë¼ë¯¸í„° ì„¤ì •
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_predict 256
PARAMETER stop "<|im_end|>"
PARAMETER stop "<|im_start|>"

# í…œí”Œë¦¿ (ChatML í˜•ì‹)
TEMPLATE """{{{{ if .System }}}}<|im_start|>system
{{{{ .System }}}}<|im_end|>
{{{{ end }}}}{{{{ if .Prompt }}}}<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
{{{{ end }}}}<|im_start|>assistant
{{{{ .Response }}}}<|im_end|>
"""
'''
    
    modelfile_path = Path(output_path) / "Modelfile"
    with open(modelfile_path, "w", encoding="utf-8") as f:
        f.write(modelfile_content)
    
    print(f"âœ… Modelfile ìƒì„±: {modelfile_path}")
    print(f"\nğŸ“ Ollama ë“±ë¡ ëª…ë ¹ì–´:")
    print(f"   ollama create {model_name} -f {modelfile_path}")
    
    return modelfile_path


def main():
    parser = argparse.ArgumentParser(description="LoRA ë³‘í•© ë° GGUF ë³€í™˜")
    
    parser.add_argument("--base_model", type=str,
                        default="kakaocorp/kanana-nano-2.1b-instruct",
                        help="ë² ì´ìŠ¤ ëª¨ë¸")
    parser.add_argument("--lora_path", type=str,
                        default="./finetuning/output/kanana-counseling-lora",
                        help="LoRA ì–´ëŒ‘í„° ê²½ë¡œ")
    parser.add_argument("--output_dir", type=str,
                        default="./finetuning/output/kanana-counseling-merged",
                        help="ë³‘í•© ëª¨ë¸ ì¶œë ¥ ê²½ë¡œ")
    parser.add_argument("--quantization", type=str,
                        default="q4_k_m",
                        choices=["f16", "q8_0", "q4_k_m", "q4_k_s", "q5_k_m"],
                        help="GGUF ì–‘ìí™” íƒ€ì…")
    parser.add_argument("--model_name", type=str,
                        default="kanana-counseling",
                        help="Ollama ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--skip_merge", action="store_true",
                        help="ë³‘í•© ë‹¨ê³„ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ ë³‘í•©ëœ ê²½ìš°)")
    parser.add_argument("--skip_gguf", action="store_true",
                        help="GGUF ë³€í™˜ ê±´ë„ˆë›°ê¸°")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ ëª¨ë¸ ë³€í™˜ ì‹œì‘")
    print("=" * 60)
    
    merged_path = args.output_dir
    
    # 1. LoRA ë³‘í•©
    if not args.skip_merge:
        merged_path = merge_lora_adapter(
            args.base_model,
            args.lora_path,
            args.output_dir
        )
    
    # 2. GGUF ë³€í™˜
    gguf_path = None
    if not args.skip_gguf:
        gguf_filename = f"{args.model_name}-{args.quantization}.gguf"
        gguf_path = str(Path(args.output_dir) / gguf_filename)
        gguf_path = convert_to_gguf(merged_path, gguf_path, args.quantization)
    
    # 3. Ollama Modelfile ìƒì„±
    if gguf_path:
        create_ollama_modelfile(gguf_path, args.output_dir, args.model_name)
    
    print("\n" + "=" * 60)
    print("âœ… ë³€í™˜ ì™„ë£Œ!")
    print("=" * 60)
    
    if gguf_path:
        print(f"\nğŸ“ GGUF íŒŒì¼: {gguf_path}")
        print("\nğŸ”§ Ollama ë“±ë¡:")
        print(f"   cd {args.output_dir}")
        print(f"   ollama create {args.model_name} -f Modelfile")
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸:")
        print(f"   ollama run {args.model_name} 'ì•ˆë…•í•˜ì„¸ìš”'")


if __name__ == "__main__":
    main()

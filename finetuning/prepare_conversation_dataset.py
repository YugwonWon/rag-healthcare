"""
ëŒ€í™” ì˜ˆì œ íŒŒì¼ë“¤ì„ íŒŒì‹±í•˜ì—¬ train/valid ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€í™˜
ë‹¤ì–‘í•œ í˜•ì‹ì˜ íŒŒì¼ë“¤ì„ í†µí•© ì²˜ë¦¬
- ê° ì‚¬ë¡€/ìƒí™©ë³„ë¡œ ê°œë³„ ëŒ€í™”ë¡œ ë¶„ë¦¬
- ë©€í‹°í„´ ëŒ€í™” ì§€ì›
"""

import os
import re
import json
import random
from pathlib import Path
from typing import List, Dict, Tuple

# ì„¤ì •
CONVERSATIONS_DIR = Path(__file__).parent.parent / "data" / "conversations"
OUTPUT_DIR = Path(__file__).parent / "data"
TRAIN_RATIO = 0.85  # 85% train, 15% valid

# ì—­í•  ë§¤í•‘ (ë‹¤ì–‘í•œ í‘œí˜„ì„ í‘œì¤€í™”)
USER_ROLES = ["ê³ ë ¹ì", "ì–´ë¥´ì‹ ", "ì´ìš©ì", "user"]
ASSISTANT_ROLES = ["ìƒë‹´ì‚¬", "ê´€ë¦¬ì‚¬", "ê±´ê°•ê´€ë¦¬ì‚¬", "agent", "assistant"]

# ê¸°ë³¸ System í”„ë¡¬í”„íŠ¸
DEFAULT_SYSTEM = "ë„ˆëŠ” ë…¸ì¸ê±´ê°•ì „ë¬¸ìƒë‹´ì‚¬ë¡œì„œ ì–´ë¥´ì‹ ì˜ ê±´ê°• ê³ ë¯¼ì— ê³µê°í•˜ë©° ì¼ìƒì—ì„œ ì‹¤ì²œí•  ìˆ˜ ìˆëŠ” ê±´ê°• ìŠµê´€ì„ ì•Œë ¤ì£¼ê³ , ì¦ìƒì´ ì‹¬ê°í•œ ê²½ìš° ì˜ì‚¬ ì§„ë£Œë¥¼ ê¶Œìœ í•œë‹¤."


def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬: ê¹¨ì§„ ë¬¸ì, ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°"""
    if not text:
        return ""
    # ê¹¨ì§„ í•œê¸€ ë¬¸ì ì œê±° (ìëª¨ìŒ ë¶„ë¦¬ëœ ë¬¸ì)
    text = re.sub(r'[á„€-á…Ÿá… -á†¿ã„±-ã…ã…-ã…£]', '', text)
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()
    # [ê²€ìƒ‰], [ë°ì´í„°ë² ì´ìŠ¤ê²€ìƒ‰] ë“± í”Œë ˆì´ìŠ¤í™€ë” ì œê±°
    text = re.sub(r'\[ê²€ìƒ‰[^\]]*\]', '', text)
    text = re.sub(r'\[ë°ì´í„°ë² ì´ìŠ¤ê²€ìƒ‰\]', '', text)
    # ?? ê°™ì€ ë¶ˆì™„ì „í•œ ë§ˆì»¤ ì œê±°
    text = re.sub(r'\?\?+', '', text)
    return text.strip()


def normalize_role(role: str) -> str:
    """ì—­í• ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    role_clean = role.strip().lower()
    
    for r in USER_ROLES:
        if r.lower() in role_clean:
            return "user"
    
    for r in ASSISTANT_ROLES:
        if r.lower() in role_clean:
            return "assistant"
    
    if "system" in role_clean:
        return "system"
    
    return role_clean


def split_by_section(content: str) -> List[str]:
    """
    ë‹¤ì–‘í•œ ì„¹ì…˜ êµ¬ë¶„ìë¡œ ì½˜í…ì¸  ë¶„ë¦¬
    - <ì œëª©> í˜•íƒœ
    - [ì‚¬ë¡€ N], [ìƒí™© N]
    - ì‚¬ë¡€ N:
    - System í”„ë¡¬í”„íŠ¸ ì‹œì‘ì 
    """
    # ì„¹ì…˜ êµ¬ë¶„ì íŒ¨í„´ë“¤
    section_patterns = [
        r'<[^>]+>',  # <ê°‘ìê¸° ì–¼êµ´ì´ í™• ë‹¬ì•„ì˜¤ë¥´ê³  ë•€ì´ ë‚¨>
        r'\[ì‚¬ë¡€\s*\d+\]',  # [ì‚¬ë¡€ 1]
        r'\[ìƒí™©\s*\d+[^\]]*\]',  # [ìƒí™© 1: ...]
        r'(?:^|\n)ì‚¬ë¡€\s*\d+\s*[:ï¼š]',  # ì‚¬ë¡€ 1:
        r'(?:^|\n)System\s*(?:\([^)]*\))?\s*[:ï¼š]',  # System:
    ]
    
    combined_pattern = '|'.join(f'({p})' for p in section_patterns)
    
    # ì„¹ì…˜ ë¶„ë¦¬
    parts = re.split(combined_pattern, content, flags=re.MULTILINE | re.IGNORECASE)
    
    sections = []
    current_section = ""
    
    for part in parts:
        if part is None:
            continue
        part = part.strip()
        if not part:
            continue
            
        # ì„¹ì…˜ êµ¬ë¶„ìì¸ ê²½ìš°
        is_separator = any(re.match(p, part, re.IGNORECASE) for p in section_patterns)
        
        if is_separator:
            if current_section.strip():
                sections.append(current_section.strip())
            current_section = part
        else:
            current_section += " " + part
    
    if current_section.strip():
        sections.append(current_section.strip())
    
    return sections if sections else [content]


def extract_turns_from_text(text: str) -> List[Dict]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ ëŒ€í™” í„´ ì¶”ì¶œ
    ì—­í• : ë©”ì‹œì§€ í˜•íƒœ ë˜ëŠ” â€¢ ì—­í• : ë©”ì‹œì§€ í˜•íƒœ ëª¨ë‘ ì²˜ë¦¬
    """
    turns = []
    
    # ì—­í•  íŒ¨í„´: "ì—­í• :" ë˜ëŠ” "â€¢ ì—­í• :"
    role_pattern = r'(?:^|\n)[â€¢Â·]?\s*(System|system|ê³ ë ¹ì|ì–´ë¥´ì‹ |ì´ìš©ì|ìƒë‹´ì‚¬|ê´€ë¦¬ì‚¬|ê±´ê°•ê´€ë¦¬ì‚¬|User|USer|user|Agent|agent|Assistant|assistant)\s*(?:\([^)]*\))?\s*[:ï¼š]\s*'
    
    # ì—­í• ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¦¬
    parts = re.split(role_pattern, text, flags=re.MULTILINE | re.IGNORECASE)
    
    # parts: [prefix, role1, msg1, role2, msg2, ...]
    for i in range(1, len(parts), 2):
        if i + 1 < len(parts):
            role = parts[i].strip()
            message = parts[i + 1].strip()
            
            # ë©”ì‹œì§€ ì •ë¦¬
            message = re.sub(r'\s+', ' ', message).strip()
            # ë‹¤ìŒ ì„¹ì…˜ ë§ˆì»¤ ì „ê¹Œì§€ë§Œ
            message = re.split(r'(?=<[^>]+>|\[ì‚¬ë¡€|\[ìƒí™©)', message)[0].strip()
            
            if message and len(message) > 2:
                normalized_role = normalize_role(role)
                turns.append({
                    "role": normalized_role,
                    "content": message
                })
    
    return turns


def parse_conversation_file(filepath: Path) -> List[Dict]:
    """
    ëŒ€í™” íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ê°œë³„ ëŒ€í™” ëª©ë¡ ë°˜í™˜
    ê° ëŒ€í™”ëŠ” {"system": str, "turns": [{"role": str, "content": str}, ...]} í˜•íƒœ
    """
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    filename = filepath.name
    conversations = []
    
    # ca_sample.txt íŠ¹ë³„ ì²˜ë¦¬ (ìƒí™© í˜•ì‹)
    if filename == "ca_sample.txt":
        situations = re.split(r'\[ìƒí™©\s*\d+[^\]]*\]', content)
        for situation in situations:
            if not situation.strip():
                continue
            turns = extract_turns_from_text(situation)
            if turns:
                # ì—°ì†ëœ ë™ì¼ ì—­í•  ì œê±°, user/assistant ë²ˆê°ˆì•„ ë‚˜ì˜¤ë„ë¡
                cleaned_turns = clean_consecutive_roles(turns)
                if cleaned_turns:
                    conversations.append({
                        "system": DEFAULT_SYSTEM,
                        "turns": cleaned_turns
                    })
        return conversations
    
    # ì†ë°œì €ë¦¼ ëŒ€í™”ì˜ˆì œ.txt íŠ¹ë³„ ì²˜ë¦¬ (ì‚¬ë¡€: ë¶ˆë¦¿ í˜•ì‹)
    if "ì†ë°œì €ë¦¼" in filename:
        cases = re.split(r'ì‚¬ë¡€\s*\d+\s*[:ï¼š]', content)
        for case in cases:
            if not case.strip():
                continue
            turns = extract_turns_from_text(case)
            if turns:
                cleaned_turns = clean_consecutive_roles(turns)
                if cleaned_turns:
                    conversations.append({
                        "system": DEFAULT_SYSTEM,
                        "turns": cleaned_turns
                    })
        return conversations
    
    # ì¼ë°˜ í˜•ì‹: System í”„ë¡¬í”„íŠ¸ ë˜ëŠ” <ì œëª©> ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¦¬
    # ë¨¼ì € <ì œëª©> íŒ¨í„´ìœ¼ë¡œ ë¶„ë¦¬ ì‹œë„
    sections = re.split(r'(<[^>]+>)', content)
    
    current_system = ""
    current_turns = []
    
    for i, section in enumerate(sections):
        section = section.strip()
        if not section:
            continue
        
        # <ì œëª©> ë§ˆì»¤ì¸ ê²½ìš° (ìƒˆ ëŒ€í™” ì‹œì‘)
        if re.match(r'<[^>]+>', section):
            # ì´ì „ ëŒ€í™” ì €ì¥
            if current_turns:
                cleaned = clean_consecutive_roles(current_turns)
                if cleaned:
                    conversations.append({
                        "system": current_system or DEFAULT_SYSTEM,
                        "turns": cleaned
                    })
            current_system = ""
            current_turns = []
            continue
        
        # System í”„ë¡¬í”„íŠ¸ íŒ¨í„´ìœ¼ë¡œ ë‹¤ì‹œ ë¶„ë¦¬
        system_parts = re.split(r'(System\s*(?:\([^)]*\))?\s*[:ï¼š])', section, flags=re.IGNORECASE)
        
        for j, part in enumerate(system_parts):
            part = part.strip()
            if not part:
                continue
            
            # System ë§ˆì»¤ì¸ ê²½ìš°
            if re.match(r'System', part, re.IGNORECASE):
                # ì´ì „ ëŒ€í™” ì €ì¥
                if current_turns:
                    cleaned = clean_consecutive_roles(current_turns)
                    if cleaned:
                        conversations.append({
                            "system": current_system or DEFAULT_SYSTEM,
                            "turns": cleaned
                        })
                current_turns = []
                continue
            
            # System í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
            system_match = re.match(r'(.+?)(?=ê³ ë ¹ì|ì–´ë¥´ì‹ |ì´ìš©ì|User|USer)', part, re.DOTALL | re.IGNORECASE)
            if system_match:
                current_system = system_match.group(1).strip()
                # System ì´í›„ í…ìŠ¤íŠ¸ì—ì„œ í„´ ì¶”ì¶œ
                remaining = part[system_match.end()-len(system_match.group(0).split()[-1]):]
                turns = extract_turns_from_text(part)
            else:
                turns = extract_turns_from_text(part)
            
            current_turns.extend(turns)
    
    # ë§ˆì§€ë§‰ ëŒ€í™” ì €ì¥
    if current_turns:
        cleaned = clean_consecutive_roles(current_turns)
        if cleaned:
            conversations.append({
                "system": current_system or DEFAULT_SYSTEM,
                "turns": cleaned
            })
    
    return conversations


def clean_consecutive_roles(turns: List[Dict]) -> List[Dict]:
    """
    ì—°ì†ëœ ë™ì¼ ì—­í•  ë©”ì‹œì§€ ë³‘í•© ë° ì •ë¦¬
    system ì—­í•  ì œê±° (ë³„ë„ë¡œ ì²˜ë¦¬)
    """
    if not turns:
        return []
    
    # system ì—­í•  ì œê±°
    turns = [t for t in turns if t["role"] in ["user", "assistant"]]
    
    if not turns:
        return []
    
    cleaned = []
    prev_role = None
    
    for turn in turns:
        role = turn["role"]
        content = clean_text(turn["content"])
        
        # ë¹ˆ ë©”ì‹œì§€ë‚˜ ë„ˆë¬´ ì§§ì€ ë©”ì‹œì§€ ê±´ë„ˆë›°ê¸°
        if not content or len(content) < 3:
            continue
        
        # ì—°ì†ëœ ë™ì¼ ì—­í• ì´ë©´ ë³‘í•©
        if role == prev_role and cleaned:
            cleaned[-1]["content"] += " " + content
        else:
            cleaned.append({"role": role, "content": content})
            prev_role = role
    
    # ì²« í„´ì´ userê°€ ì•„ë‹ˆë©´ ì œê±°
    while cleaned and cleaned[0]["role"] != "user":
        cleaned.pop(0)
    
    # ìµœì†Œ 1í„´ ì´ìƒ (user -> assistant)
    if len(cleaned) < 2:
        return []
    
    # userì™€ assistant ë‘˜ ë‹¤ ìˆëŠ”ì§€ í™•ì¸
    has_user = any(t["role"] == "user" for t in cleaned)
    has_assistant = any(t["role"] == "assistant" for t in cleaned)
    
    if not (has_user and has_assistant):
        return []
    
    return cleaned


def parse_file(filepath: Path) -> List[Dict]:
    """íŒŒì¼ í˜•ì‹ì— ë”°ë¼ ì ì ˆí•œ íŒŒì„œ ì„ íƒ"""
    return parse_conversation_file(filepath)


def create_chat_format(conv: Dict) -> Dict:
    """
    ëŒ€í™”ë¥¼ ChatML í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    """
    messages = []
    
    # System ë©”ì‹œì§€ëŠ” í•­ìƒ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì¼ê´€ì„±ì„ ìœ„í•´)
    messages.append({"role": "system", "content": DEFAULT_SYSTEM})
    
    # ëŒ€í™” í„´
    for turn in conv.get("turns", []):
        content = clean_text(turn["content"])
        if content and len(content) > 2:
            messages.append({
                "role": turn["role"],
                "content": content
            })
    
    return {"messages": messages}


def validate_conversation(conv: Dict) -> bool:
    """ëŒ€í™” ìœ íš¨ì„± ê²€ì¦"""
    messages = conv.get("messages", [])
    
    # ìµœì†Œ ì¡°ê±´: system + user + assistant = 3ê°œ ì´ìƒ
    if len(messages) < 3:
        return False
    
    # userì™€ assistant ê°ê° 1ê°œ ì´ìƒ
    has_user = any(m["role"] == "user" for m in messages)
    has_assistant = any(m["role"] == "assistant" for m in messages)
    
    return has_user and has_assistant


def split_multi_turn_conversations(conversations: List[Dict], max_turns: int = 6) -> List[Dict]:
    """
    ê¸´ ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì—¬ëŸ¬ ê°œì˜ ì§§ì€ ëŒ€í™”ë¡œ ë¶„ë¦¬
    íŒŒì¸íŠœë‹ ë°ì´í„° ì¦ê°•ì„ ìœ„í•´
    """
    result = []
    
    for conv in conversations:
        system = conv.get("system", DEFAULT_SYSTEM)
        turns = conv.get("turns", [])
        
        if len(turns) <= max_turns:
            result.append(conv)
        else:
            # ê¸´ ëŒ€í™”ë¥¼ ì—¬ëŸ¬ ê°œë¡œ ë¶„ë¦¬
            # ê²¹ì¹˜ëŠ” ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë¶„ë¦¬
            for i in range(0, len(turns), max_turns - 2):
                chunk = turns[i:i + max_turns]
                
                # ì²« í„´ì´ userì¸ì§€ í™•ì¸
                if chunk and chunk[0]["role"] == "user":
                    result.append({
                        "system": system,
                        "turns": chunk
                    })
    
    return result


def main():
    print("=" * 60)
    print("ëŒ€í™” ì˜ˆì œ ë°ì´í„°ì…‹ ìƒì„±")
    print("=" * 60)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    all_conversations = []
    file_stats = {}
    
    # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
    for filepath in sorted(CONVERSATIONS_DIR.glob("*.txt")):
        print(f"\nğŸ“„ ì²˜ë¦¬ ì¤‘: {filepath.name}")
        
        try:
            convs = parse_file(filepath)
            print(f"   íŒŒì‹±ëœ ëŒ€í™”: {len(convs)}ê°œ")
            
            # ê¸´ ëŒ€í™” ë¶„ë¦¬
            convs = split_multi_turn_conversations(convs, max_turns=8)
            print(f"   ë¶„ë¦¬ í›„: {len(convs)}ê°œ")
            
            # ChatML í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            chat_convs = []
            for conv in convs:
                chat_conv = create_chat_format(conv)
                if validate_conversation(chat_conv):
                    chat_convs.append(chat_conv)
            
            print(f"   ìœ íš¨í•œ ëŒ€í™”: {len(chat_convs)}ê°œ")
            
            file_stats[filepath.name] = {
                "parsed": len(convs),
                "valid": len(chat_convs)
            }
            
            all_conversations.extend(chat_convs)
            
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\nì´ ìœ íš¨í•œ ëŒ€í™”: {len(all_conversations)}ê°œ")
    
    # ì…”í”Œ í›„ train/valid ë¶„ë¦¬
    random.seed(42)
    random.shuffle(all_conversations)
    
    split_idx = int(len(all_conversations) * TRAIN_RATIO)
    train_data = all_conversations[:split_idx]
    valid_data = all_conversations[split_idx:]
    
    print(f"\nğŸ“Š ë°ì´í„°ì…‹ ë¶„í• :")
    print(f"   Train: {len(train_data)}ê°œ")
    print(f"   Valid: {len(valid_data)}ê°œ")
    
    # JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
    train_path = OUTPUT_DIR / "healthcare_conversations_train.jsonl"
    valid_path = OUTPUT_DIR / "healthcare_conversations_valid.jsonl"
    
    with open(train_path, 'w', encoding='utf-8') as f:
        for conv in train_data:
            f.write(json.dumps(conv, ensure_ascii=False) + '\n')
    
    with open(valid_path, 'w', encoding='utf-8') as f:
        for conv in valid_data:
            f.write(json.dumps(conv, ensure_ascii=False) + '\n')
    
    print(f"\nâœ… ì €ì¥ ì™„ë£Œ:")
    print(f"   {train_path}")
    print(f"   {valid_path}")
    
    # ìƒ˜í”Œ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 3ê°œ):")
    print("=" * 60)
    
    for idx, sample in enumerate(train_data[:3]):
        print(f"\n--- ìƒ˜í”Œ {idx + 1} ---")
        for msg in sample["messages"]:
            role = msg["role"].upper()
            content = msg["content"][:80] + "..." if len(msg["content"]) > 80 else msg["content"]
            print(f"[{role}] {content}")
    
    # í†µê³„ ì¶œë ¥
    print("\n" + "=" * 60)
    print("íŒŒì¼ë³„ í†µê³„:")
    print("=" * 60)
    total_valid = 0
    for filename, stats in sorted(file_stats.items()):
        print(f"  {filename}: {stats['parsed']}ê°œ íŒŒì‹± â†’ {stats['valid']}ê°œ ìœ íš¨")
        total_valid += stats['valid']
    
    print(f"\n  í•©ê³„: {total_valid}ê°œ")


if __name__ == "__main__":
    main()

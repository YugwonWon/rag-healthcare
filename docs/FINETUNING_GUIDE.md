# íŒŒì¸íŠœë‹ ê°€ì´ë“œ

Qwen 2.5 3B ëª¨ë¸ì„ ì¹˜ë§¤ë…¸ì¸-ìƒí™œì§€ì›ì‚¬ ëŒ€í™” ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹í•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´

- **GPU**: NVIDIA GPU (ìµœì†Œ 16GB VRAM ê¶Œì¥)
  - RTX 3090/4090, A100, H100 ë“±
  - QLoRA ì‚¬ìš© ì‹œ 8GB VRAMìœ¼ë¡œë„ ê°€ëŠ¥
- **RAM**: 32GB ì´ìƒ ê¶Œì¥
- **Storage**: 50GB ì´ìƒ ì—¬ìœ  ê³µê°„

### ì†Œí”„íŠ¸ì›¨ì–´

```bash
# CUDA 11.8 ì´ìƒ
nvidia-smi

# íŒŒì¸íŠœë‹ ì˜ì¡´ì„± ì„¤ì¹˜
cd finetuning
pip install -r requirements.txt
```

## ğŸ“‚ ë°ì´í„° ì¤€ë¹„

### 1. ë°ì´í„° í˜•ì‹

ëŒ€í™” ë°ì´í„°ëŠ” JSONL í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„í•©ë‹ˆë‹¤:

```jsonl
{
  "id": "conv_001",
  "patient_info": "80ì„¸ ì—¬ì„±, ê²½ë„ ì¹˜ë§¤",
  "dialogue": [
    {"speaker": "patient", "text": "ì˜¤ëŠ˜ ì•½ ë¨¹ì—ˆë‚˜?"},
    {"speaker": "caregiver", "text": "ë„¤, ì–´ë¥´ì‹ . ì•„ì¹¨ì— ë“œì…¨ì–´ìš”."}
  ]
}
```

### 2. ìƒ˜í”Œ ë°ì´í„° ìƒì„±

```bash
python prepare_dataset.py --create-sample
```

### 3. ì‹¤ì œ ë°ì´í„° ë³€í™˜

```bash
python prepare_dataset.py \
    --input ./data/raw/conversations.jsonl \
    --output ./data/conversations \
    --format chat
```

ì¶œë ¥ íŒŒì¼:
- `train_chat.jsonl`: í•™ìŠµ ë°ì´í„°
- `val_chat.jsonl`: ê²€ì¦ ë°ì´í„°

## ğŸš€ íŒŒì¸íŠœë‹ ì‹¤í–‰

### ê¸°ë³¸ ì‹¤í–‰ (QLoRA)

```bash
python train_lora.py \
    --model_name Qwen/Qwen2.5-3B-Instruct \
    --train_data ./data/conversations/train_chat.jsonl \
    --val_data ./data/conversations/val_chat.jsonl \
    --output_dir ./outputs/qwen-healthcare-lora \
    --num_epochs 3 \
    --batch_size 4 \
    --use_4bit
```

### ê³ ê¸‰ ì„¤ì •

```bash
python train_lora.py \
    --model_name Qwen/Qwen2.5-3B-Instruct \
    --train_data ./data/conversations/train_chat.jsonl \
    --output_dir ./outputs/qwen-healthcare-lora \
    --num_epochs 5 \
    --batch_size 2 \
    --gradient_accumulation_steps 8 \
    --learning_rate 1e-4 \
    --lora_r 32 \
    --lora_alpha 64 \
    --use_4bit
```

### LoRA íŒŒë¼ë¯¸í„° ì„¤ëª…

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|--------|------|
| `lora_r` | 16 | LoRA rank (ë†’ì„ìˆ˜ë¡ í‘œí˜„ë ¥â†‘, ë©”ëª¨ë¦¬â†‘) |
| `lora_alpha` | 32 | LoRA ìŠ¤ì¼€ì¼ë§ (ë³´í†µ rì˜ 2ë°°) |
| `lora_dropout` | 0.05 | ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ |
| `target_modules` | ì „ì²´ | ì ìš©í•  ë ˆì´ì–´ |

## ğŸ“Š ëª¨ë¸ í‰ê°€

### í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€

```bash
python eval_model.py \
    --model_path Qwen/Qwen2.5-3B-Instruct \
    --adapter_path ./outputs/qwen-healthcare-lora \
    --test_data ./data/conversations/val_chat.jsonl \
    --output eval_results.json \
    --use_4bit
```

### ëŒ€í™”í˜• í‰ê°€

```bash
python eval_model.py \
    --model_path Qwen/Qwen2.5-3B-Instruct \
    --adapter_path ./outputs/qwen-healthcare-lora \
    --interactive \
    --use_4bit
```

## ğŸ”€ ì–´ëŒ‘í„° ë³‘í•©

### LoRA ì–´ëŒ‘í„° ë³‘í•©

```bash
python merge_adapter.py merge \
    --base_model Qwen/Qwen2.5-3B-Instruct \
    --adapter ./outputs/qwen-healthcare-lora \
    --output ./outputs/qwen-healthcare-merged
```

### HuggingFace Hub ì—…ë¡œë“œ

```bash
python merge_adapter.py merge \
    --base_model Qwen/Qwen2.5-3B-Instruct \
    --adapter ./outputs/qwen-healthcare-lora \
    --output ./outputs/qwen-healthcare-merged \
    --push_to_hub \
    --hub_model_id your-username/qwen-healthcare
```

## ğŸ–¥ï¸ Ollama ì—°ë™

### GGUF ë³€í™˜ (llama.cpp ì‚¬ìš©)

```bash
# llama.cpp ì„¤ì¹˜
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make

# ë³€í™˜
python convert-hf-to-gguf.py ../outputs/qwen-healthcare-merged \
    --outfile qwen-healthcare.gguf \
    --outtype q4_k_m
```

### Ollama Modelfile ìƒì„±

```bash
python merge_adapter.py modelfile \
    --model_path ./qwen-healthcare.gguf \
    --output Modelfile \
    --name qwen-healthcare
```

### Ollamaì— ë“±ë¡

```bash
ollama create qwen-healthcare -f Modelfile
ollama run qwen-healthcare
```

## ğŸ’¡ íŒ & íŠ¸ë¦­

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ

1. `--use_4bit` ì˜µì…˜ ì‚¬ìš©
2. `--batch_size` ì¤„ì´ê¸°
3. `--gradient_accumulation_steps` ëŠ˜ë¦¬ê¸°

### í•™ìŠµ ëª¨ë‹ˆí„°ë§

```bash
# Tensorboard ì‹¤í–‰
tensorboard --logdir ./outputs/qwen-healthcare-lora

# ë˜ëŠ” Wandb ì‚¬ìš©
pip install wandb
wandb login
# train_lora.pyì—ì„œ report_to="wandb" ì„¤ì •
```

### ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ

```bash
python train_lora.py \
    --model_name Qwen/Qwen2.5-3B-Instruct \
    --train_data ./data/conversations/train_chat.jsonl \
    --output_dir ./outputs/qwen-healthcare-lora \
    --resume_from_checkpoint
```

## ğŸ“ˆ ì˜ˆìƒ í•™ìŠµ ì‹œê°„

| GPU | ë°ì´í„° í¬ê¸° | QLoRA | ì˜ˆìƒ ì‹œê°„ |
|-----|------------|-------|----------|
| RTX 3090 | 1,000 ìƒ˜í”Œ | âœ… | ~30ë¶„ |
| RTX 3090 | 10,000 ìƒ˜í”Œ | âœ… | ~5ì‹œê°„ |
| A100 40GB | 10,000 ìƒ˜í”Œ | âŒ | ~3ì‹œê°„ |

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### CUDA Out of Memory

```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
--batch_size 1 --gradient_accumulation_steps 16

# ë” ê³µê²©ì ì¸ ì–‘ìí™”
--use_4bit
```

### í•™ìŠµì´ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ

- Learning rate ì¡°ì •: `1e-4` â†’ `2e-5`
- LoRA rank ëŠ˜ë¦¬ê¸°: `16` â†’ `32`
- ë” ë§ì€ ì—í­: `3` â†’ `5`

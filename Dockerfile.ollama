# ì¹˜ë§¤ë…¸ì¸ ë§ì¶¤í˜• í—¬ìŠ¤ì¼€ì–´ RAG ì±—ë´‡ - Cloud Run ë°°í¬ìš© (Ollama í¬í•¨)
# í™˜ê²½ë³€ìˆ˜ OLLAMA_MODELë¡œ ëª¨ë¸ì„ ì§€ì •í•˜ë©´ ìë™ìœ¼ë¡œ í•´ë‹¹ ëª¨ë¸ì„ ë“±ë¡/ì‚¬ìš©
#
# ì‚¬ìš©ë²•:
#   docker build --build-arg OLLAMA_MODEL=k-exaone-counseling -t healthcare-rag .
#   ë˜ëŠ” deploy_cloudrun.sh ì‹¤í–‰

FROM python:3.12-slim

# Build argument: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: k-exaone-counseling)
ARG OLLAMA_MODEL=k-exaone-counseling

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    LOG_LEVEL=INFO \
    LOG_TO_CONSOLE=true \
    LOG_TO_FILE=true \
    OLLAMA_HOST=0.0.0.0:11434 \
    OLLAMA_MODELS=/app/ollama_models \
    OLLAMA_BASE_URL=http://localhost:11434 \
    OLLAMA_MODEL=${OLLAMA_MODEL} \
    OLLAMA_DEBUG=0 \
    OLLAMA_FLASH_ATTENTION=1 \
    PYTHONIOENCODING=utf-8

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± + Ollama ì„¤ì¹˜
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    curl \
    ca-certificates \
    locales \
    zstd \
    && echo "en_US.UTF-8 UTF-8" > /etc/locale.gen \
    && locale-gen en_US.UTF-8 \
    && rm -rf /var/lib/apt/lists/*

# UTF-8 ë¡œì¼€ì¼ ì„¤ì • (í•œê¸€ì€ Pythonì—ì„œ ì²˜ë¦¬)
ENV LANG=en_US.UTF-8 \
    LC_ALL=en_US.UTF-8

# Ollama ì„¤ì¹˜ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
RUN for i in 1 2 3 4 5; do \
        echo "Ollama ì„¤ì¹˜ ì‹œë„ $i/5..." && \
        curl -fsSL --retry 5 --retry-delay 10 --retry-max-time 300 https://ollama.com/install.sh -o /tmp/install.sh && \
        chmod +x /tmp/install.sh && \
        /tmp/install.sh && \
        rm /tmp/install.sh && \
        break || \
        (echo "ì‹œë„ $i ì‹¤íŒ¨, 10ì´ˆ í›„ ì¬ì‹œë„..." && sleep 10); \
    done && \
    ollama --version

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY app/ ./app/
COPY data/ ./data/
COPY scripts/ ./scripts/

# ëª¨ë¸ ë””ë ‰í† ë¦¬ ë³µì‚¬ (GGUF + Modelfile í¬í•¨)
# models/ ë””ë ‰í† ë¦¬ì— {ëª¨ë¸ëª…}.gguf ì™€ Modelfile.{ëª¨ë¸ëª…} ì´ ìˆì–´ì•¼ í•¨
COPY models/ ./models/

# ë””ë ‰í† ë¦¬ ìƒì„± ë° ìŠ¤í¬ë¦½íŠ¸ ê¶Œí•œ
RUN mkdir -p /app/logs /app/ollama_models && \
    chmod +x ./scripts/start_with_ollama.sh

# í¬íŠ¸ ë…¸ì¶œ (Cloud Runì€ PORT=8080 ì£¼ì…)
EXPOSE 8080

# ë¹Œë“œ ì‹œ ëª¨ë¸ ì‚¬ì „ ë“±ë¡ (Cold Start ì‹œê°„ ë‹¨ì¶•)
# Ollama ì„œë²„ë¥¼ ì ì‹œ ë„ìš°ê³ , ëª¨ë¸ create í›„ ì¢…ë£Œ
RUN if [ -f "./models/Modelfile.${OLLAMA_MODEL}" ]; then \
        echo "ğŸ“¦ Pre-registering model: ${OLLAMA_MODEL}" && \
        ollama serve &  \
        sleep 3 && \
        cd /app/models && \
        ollama create ${OLLAMA_MODEL} -f Modelfile.${OLLAMA_MODEL} && \
        cd /app && \
        echo "âœ… Model pre-registered!" && \
        kill %1 2>/dev/null; \
    elif [ -f "./models/${OLLAMA_MODEL}.gguf" ]; then \
        echo "ğŸ“¦ Pre-registering model from GGUF: ${OLLAMA_MODEL}" && \
        printf "FROM ./models/${OLLAMA_MODEL}.gguf\nSYSTEM \"ë‹¹ì‹ ì€ ë…¸ì¸ê±´ê°•ì „ë¬¸ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.\"\nPARAMETER temperature 0.1\n" > /tmp/Modelfile.auto && \
        ollama serve & \
        sleep 3 && \
        ollama create ${OLLAMA_MODEL} -f /tmp/Modelfile.auto && \
        rm -f /tmp/Modelfile.auto && \
        echo "âœ… Model pre-registered!" && \
        kill %1 2>/dev/null; \
    else \
        echo "âš ï¸ No local model files, will pull at runtime"; \
    fi && sleep 2 || true

# í—¬ìŠ¤ì²´í¬ (ì•±ì´ ì¤€ë¹„ë˜ë©´ /health ì‘ë‹µ)
HEALTHCHECK --interval=30s --timeout=30s --start-period=300s --retries=5 \
    CMD curl -f http://localhost:${PORT:-8080}/health || exit 1

# ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (Ollama ì‹œì‘ â†’ ëª¨ë¸ ë¡œë“œ â†’ ì•± ì‹¤í–‰)
CMD ["./scripts/start_with_ollama.sh"]
